{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":7923451,"sourceType":"datasetVersion","datasetId":4633221},{"sourceId":11371,"sourceType":"modelInstanceVersion","modelInstanceId":5171},{"sourceId":11372,"sourceType":"modelInstanceVersion","modelInstanceId":5388},{"sourceId":11373,"sourceType":"modelInstanceVersion","modelInstanceId":5391},{"sourceId":11375,"sourceType":"modelInstanceVersion","modelInstanceId":5172}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div class=\"heading\">Explaining Data Science Concepts with Gemma</div>\n<div class=\"sub_heading\">Fine-tuning Gemma and RAG with LlamaIndex and Pinecone</div>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"<br>\n<center><img src=\"https://i.ibb.co/J7291t1/kaggle-9.png\" alt=\"kaggle-6\" border=\"0\" width=800></center>\n<center>LLM Tools (Image by Author)</center>\n<br>","metadata":{}},{"cell_type":"code","source":"from IPython.display import display, HTML, Markdown\n\n\n# Styling table of contents\nhtml_contents =\"\"\"\n<!DOCTYPE html>\n\n<style>\nmark {\n    background-color: #00AF01;\n    color: white;\n    padding: 2px 4px; /* Optional: adds padding around the text */\n    font-family: monospace; /* Optional: gives the text a typewriter-like appearance */\n    font-size: 14px; /* Adjust the font size as desired */\n}\n\n.marks {\n    font-size: 12px; /* Smaller font size */\n    background-color: #00AF01; /* Example background color */\n    color: white; /* Text color */\n    padding: 2px 4px; /* Padding around the text */\n    font-family: monospace; /* Typewriter-like appearance */\n}\n\n.text-with-green-strip {\n    border-left: 30px solid #A0FFA1; /* Creates the grey strip */\n    padding-left: 0px; /* Adds some space between the strip and the text */\n    margin-left: 60px; /* Optional: Adds space to the left of the strip */\n    /* Additional styling for the text block */\n    font-size: 14px;\n    line-height: 1.6;\n    color: #333; /* Text color */\n}\n\n.grey-strip {\n    background-color: #A0FFA1; /* Set the background color */\n    width: 100%; /* Set the width */\n    height: 20px; /* Set the height */\n    margin: 20px 0; /* Optional: adds some space above and below the strip */\n}\n\n    /* Apply font size directly to list and list items */\n    ul, li {\n        font-size: medium; /* Ensure consistency */\n    }\n       \n.custom-ul {\n    font-size: 14x;\n    margin-left: 90px;\n}   \n\n.custom-ul li {\n    list-style-type: disc; /* Changes bullet points to disc */\n    font-size: 14px;\n}\n\n</style>\n\n<html lang=\"en\">\n    <head>\n    <style>\n    .toc h2{\n        color: white;\n        background: #005A0A;\n        font-weight: 600;\n        font-family: Helvetica;\n        font-size: 23px;\n        padding: 6px 12px;\n        margin-bottom: 2px;\n    }\n    \n    .toc ol li{\n        list-style:none;\n        line-height:normal;\n        }\n     \n    .toc li{\n        background: #46A050;\n        color: white;\n        font-weight: 600;\n        font-family: Helvetica;\n        font-size: 19px;\n        margin-bottom: 2px;\n        padding: 6px 12px;\n    }\n\n    .toc ol ol li{\n        background: white;\n        color: #46A050;\n        font-weight: 400;\n        font-size: 18px;\n        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n        margin-top: 0px;\n        margin-bottom: 0px;\n        padding: 3px 12px;\n    }\n    .section_title{\n        background-color: #005A0A;\n        color: white;\n        font-family: Helvetica;\n        font-size: 25px;\n        padding: 6px 12px;\n        margin-bottom: 5px;\n    }\n    .subsection_title{\n        background: white;\n        color: #46A050;\n        font-family: Helvetica;\n        font-size: 23px;\n        padding: 24px 12px;\n        margin-bottom: -30px;\n    }\n    .heading{\n        background: #005A0A;\n        color: white;\n        font-family: Helvetica;\n        font-size: 33px;\n        padding: 6px 12px;\n        margin-bottom: 5px;\n    }\n    .sub_heading{\n        background: #46A050;\n        color: white;\n        font-family: Helvetica;\n        font-size: 25px;\n        padding: 6px 12px;\n        margin-bottom: 5px;\n    }\n    \n    </style>\n    </head>\n    <body>\n        <div class=\"toc\">\n        <ol id=\"table_of_contents\">\n        <h2 style=\"color: white;\">Table of Contents</h2>\n        <li><a href=\"#1\" style=\"color: white;\">1. Introduction</a></li>\n        <li><a href=\"#2\" style=\"color: white;\">2. Gemma Models</a></li>\n        <ol> \n            <li><a href=\"#3\" style=\"color: #46A050;\">2.1. Preparation for Gemma</a></li>\n            <li><a href=\"#4\" style=\"color: #46A050;\">2.2. Pretrained Gemma Model from Keras Library</a></li>\n        </ol>\n        <li><a href=\"#5\" style=\"color: white;\">3. Fine-tuning Gemma</a></li>\n        <ol>\n            <li><a href=\"#6\" style=\"color: #46A050;\">3.1. Preparation for Fine-tuning</a></li>\n            <li><a href=\"#7\" style=\"color: #46A050;\">3.2. Data</a></li>\n            <li><a href=\"#8\" style=\"color: #46A050;\">3.3. Comparable Architectures of the Model</a></li>\n            <li><a href=\"#9\" style=\"color: #46A050;\">3.4. Fine-tuning</a></li>\n        </ol>  \n        <li><a href=\"#10\" style=\"color: white;\">4. RAG with LlamaIndex</a></li>\n        <ol>\n            <li><a href=\"#11\" style=\"color: #46A050;\">4.1. Preparation for LlamaIndex</a></li>\n            <li><a href=\"#12\" style=\"color: #46A050;\">4.2. Gemma as a Custom LLM for LlamaIndex</a></li>\n            <li><a href=\"#13\" style=\"color: #46A050;\">4.3. Embeddings and LlamaIndex Settings</a></li>\n            <li><a href=\"#14\" style=\"color: #46A050;\">4.4. Workflow of Response Generation with RAG and Gemma</a></li>\n            <li><a href=\"#15\" style=\"color: #46A050;\">4.5. Extract Keywords</a></li>\n            <li><a href=\"#16\" style=\"color: #46A050;\">4.6. Search for Keywords in Wikipedia</a></li>\n            <li><a href=\"#17\" style=\"color: #46A050;\">4.7. Create and Load Documents</a></li>\n            <li><a href=\"#18\" style=\"color: #46A050;\">4.8. Create the Index and Vector Store</a></li>\n            <li><a href=\"#19\" style=\"color: #46A050;\">4.9. Inserting Data</a></li>\n            <li><a href=\"#20\" style=\"color: #46A050;\">4.10. Creating the Query Engine</a></li>\n            <li><a href=\"#21\" style=\"color: #46A050;\">4.11. Displaying Responses and Relevancy Evaluation</a></li>\n            <li><a href=\"#22\" style=\"color: #46A050;\">4.12. Handling Queries and Generating Responses</a></li>\n        </ol> \n        <li><a href=\"#23\" style=\"color: white;\">5. Conclusion</a></li>\n        </ol>\n        </div>\n    </body>\n</html>\n\"\"\"\n\nHTML(html_contents)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-04-09T05:11:49.387222Z","iopub.execute_input":"2024-04-09T05:11:49.387614Z","iopub.status.idle":"2024-04-09T05:11:49.410037Z","shell.execute_reply.started":"2024-04-09T05:11:49.387582Z","shell.execute_reply":"2024-04-09T05:11:49.408924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n <div class=\"section_title\">\n    <a href=\"#table_of_contents\" style=\"color: white;\" >1. Introduction</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\n    This notebook aims to <b>Explain or Teach Basic Data Science Concepts</b> with <span style=\"color: #00AF01;\">Gemma</span>.<br>\n<br>\n    <span style=\"color: #00AF01;\">Gemma</span> is a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models. Developed by Google DeepMind and other teams across Google, <span style=\"color: #00AF01;\">Gemma</span> is inspired by <b>Gemini</b>, and the name reflects the Latin <i>gemma</i>, meaning “precious stone.”<br>\nSource: <a href=\"https://blog.google/technology/developers/gemma-open-models/\" target=\"_blank\">Gemma: Introducing new state-of-the-art open models</a><br>\n<br>\n    We will start with a baseline, and methodically develop the model step-by-step to cultivate a more accurate and robust version.<br>\n<br>\nIn this notebook, we delve into diverse methodologies to employ <span style=\"color: #00AF01;\">Gemma</span> for elucidating data science concepts. Our exploration encompasses three strategies: <i>Untuned Model</i>, <i>Custom Fine-tuning</i>, and a synthesis of <i>RAG with Fine-tuning</i>. We will illustrate these strategies through the following detailed approaches:<br>\n<br>\n<b>1. Untuned <span style=\"color: #00AF01;\">Gemma (gemma_2b_en)</span></b>: Employing the baseline, untuned version of <span style=\"color: #00AF01;\">Gemma</span>, we assess its efficacy in elucidating data science concepts.<br>\n<b>2. <span style=\"color: #00AF01;\">Fine-Tuning</span> with Custom Dataset</b>: We will <span style=\"color: #00AF01;\">fine-tune</span> the base <span style=\"color: #00AF01;\">Gemma (gemma_2b_en)</span> model using a specially curated dataset on data science concepts. The dataset, accompanied by details regarding its compilation, is provided.<br>\n<b>3. RAG with <span style=\"color: #00AF01;\">Gemma (gemma_2b_en)</span> and <span style=\"color: #00AF01;\">LlamaIndex</span></b>: Leveraging Wikipedia pages as data sources, the fine-tuned <span style=\"color: #00AF01;\">Gemma</span> amalgamates and presents responses from the respective sources.<br>\n<br>\nWe will ultimately evaluate the results produced by these three methodologies, and compare how each explains a particular data science concept.<br>\n<br>\nThe queries chosen for this investigation are intended for various demonstrative purposes, which will be elaborated on later. Currently, the queries under scrutiny include:<br>","metadata":{}},{"cell_type":"markdown","source":"<ul>\n    <li><b>Query-1</b>: \"What is an outlier?\"</li>\n    <li><b>Query-2</b>: \"What is Fuzzy Logic?\"</li> <!-- Corrected this line -->\n    <li><b>Query-3</b>: \"Compare and contrast zero-shot learning, one-shot learning, and few-shot learning.\"</li>\n    <li><b>Query-4</b>: \"Transfer Learning is...\"</li>\n    <li><b>Query-5</b>: \"Classification is a supervised learning application. Clustering is an unsupervised learning application. I will use K-Means for market segmentation. Please tell me what type of machine learning K-Means is.\"</li>\n    <li><b>Query-6</b>: \"Explain Brownian Motion.\"</li>\n    <li><b>Query-7</b>: \"What frequency would it tune into if I blew air on your face?!\"</li>\n</ul>","metadata":{}},{"cell_type":"code","source":"%%capture\n\n\nfrom IPython.utils import io\nimport warnings\n\n\n# Ignore all warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:11:49.411844Z","iopub.execute_input":"2024-04-09T05:11:49.412169Z","iopub.status.idle":"2024-04-09T05:11:49.418767Z","shell.execute_reply.started":"2024-04-09T05:11:49.412143Z","shell.execute_reply":"2024-04-09T05:11:49.417887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\n    First, we import the <mark>io</mark> and <mark>warnings</mark> libraries to suppress progress bar and warning messages in the output cell. The <mark>IPython.display</mark> module will help us present our results beautifully.<br>\n<br>\nOur queries as mentioned are:\n</span>","metadata":{}},{"cell_type":"code","source":"query_1 = \"What is an outlier?\"\nquery_2 = \"What is Fuzzy Logic?\"\nquery_3 = \"Compare and contrast zero-shot learning, one-shot learning, and few-shot learning.\" \nquery_4 = \"Transfer Learning is...\"\nquery_5 = \"Classification is a supervised learning application. Clustering is an unsupervised learning application. I will use K-Means for market segmentation. Please tell me what type of machine learning K-Means is.\"\nquery_6 = \"Explain Brownian Motion.\"\nquery_7 = \"What frequency would it tune into if I blew air on your face?!\"\n\n# Put all queries in a list, long version\nqueries = [query_1, query_2, query_3, query_4, query_5, query_6, query_7]\n\n# Short version\n# queries = [query_1, query_2, query_6]","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:11:49.419983Z","iopub.execute_input":"2024-04-09T05:11:49.420336Z","iopub.status.idle":"2024-04-09T05:11:49.430117Z","shell.execute_reply.started":"2024-04-09T05:11:49.420300Z","shell.execute_reply":"2024-04-09T05:11:49.429243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<div class=\"section_title\">\n    <a href=\"#table_of_contents\" style=\"color: white;\" >2. Gemma Models</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<center><img src=\"https://i.ibb.co/qrp3knt/gemma-keras.png\" alt=\"gemma-keras\" border=\"0\" width=400></center>\n<center>Gemma from Keras (Image by Author)</center>\n<br><br>\n<span style=\"font-size:medium;\">\nGemma models come in two sizes, offering flexibility to align with your computing resources, desired capabilities, and deployment environments. Each model variant is accessible in both tuned and untuned formats:\n</span>\n<ul>\n    <li><b>Pretrained Version</b>: This iteration hasn't been specialized for any particular tasks or instructions beyond the initial training with the Gemma core dataset. It's generally advisable to fine-tune this version before use.</li>\n    <li><b>Instruction Tuned Version</b>: Enhanced through training on human language interactions, this model variant is adept at engaging in conversations, functioning similarly to a basic chatbot.</li>\n</ul>\n<span style=\"font-size:medium;\">\nIf you're deliberating on which model to experiment with first, the <b>Gemma 2B</b> is a solid starting point. Its relatively lower parameter count translates to reduced computational demands and greater versatility in deployment options compared to the larger <b>Gemma 7B</b> model.<br>\nTherefore, based on parameter size and tuning status, four versions of the Gemma model are available:\n</span>","metadata":{}},{"cell_type":"markdown","source":"<center><img src=\"https://i.ibb.co/ykGdZyf/gemma-models.png\" alt=\"gemma-models\" border=\"0\" width=900></center>\n<center>Gemma Models</center>\n<span style=\"font-size:medium;\">\nSource: <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma\" target=\"_blank\">Use Gemma open models</a><br>\n</span>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nWe plan to execute the untuned version (<b>gemma_2b_en</b>) model. This version offers increased speed and reduced memory usage. Regardless of the model chosen, the practices outlined in the notebook apply to them as well. Therefore, opting for <b>gemma_2b_en</b> appears to be a reasonable choice.<br>\n</span>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<div class=\"subsection_title\">\n    <a href=\"#table_of_contents\" style=\"color: #46A050;\" >2.1. Preparation for Gemma</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nInstall necessary libraries:<br>\n<br>\n    <mark>!pip install -q -U keras-nlp</mark> installs or updates the <b>keras-nlp</b> package to the latest version available. <b>keras-nlp</b> is designed to extend the <b>Keras</b> library, providing specialized support and utilities for Natural Language Processing (NLP).<br>\n<br>\n    <mark>!pip install -q -U keras>=3</mark> installs or updates the <b>keras</b> package to the latest version available, with a minimum version requirement of 3. <b>Keras</b> is a high-level neural networks API, written in Python.\n</span>","metadata":{}},{"cell_type":"code","source":"%%capture\n\n\n!pip install -q -U keras-nlp\n!pip install -q -U keras>=3","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-09T05:11:49.431313Z","iopub.execute_input":"2024-04-09T05:11:49.431755Z","iopub.status.idle":"2024-04-09T05:12:22.098007Z","shell.execute_reply.started":"2024-04-09T05:11:49.431722Z","shell.execute_reply":"2024-04-09T05:12:22.096785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nDefine parameters:\n</span>","metadata":{}},{"cell_type":"code","source":"# Dictionary to store models' outputs\nMODELS_DICT = {}\n\n# If the value is 0, 'gemma_2b_en' will be used throughout the notebook.\n# If the value is 1, 'gemma_7b_en' will be used throughout the notebook.\nSELECT_MODEL = 0\n\n# Gemma models from Keras\nPRETRAINED_KERAS_MODEL_2B = \"gemma_2b_en\"\nPRETRAINED_KERAS_MODEL_7B = \"gemma_7b_en\"\nKERAS_GEMMA_MODELS = [PRETRAINED_KERAS_MODEL_2B, PRETRAINED_KERAS_MODEL_7B]\n\n# Selected Gemma model\nSELECTED_MODEL = KERAS_GEMMA_MODELS[SELECT_MODEL]\n\n# Parameters\nOUTPUT_MAX_LEN = 256\nNUM_BEAMS = 5","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:12:22.101525Z","iopub.execute_input":"2024-04-09T05:12:22.101880Z","iopub.status.idle":"2024-04-09T05:12:22.107884Z","shell.execute_reply.started":"2024-04-09T05:12:22.101829Z","shell.execute_reply":"2024-04-09T05:12:22.106874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\n    By setting <mark>OUTPUT_MAX_LEN</mark>, we will limit the response token size to <i>256</i>. This size is sufficiently large for explaining concepts without resulting in verbosity. <mark>NUM_BEAMS</mark> pertains to the choice of sampling method. We favor the <b>BeamSampler</b> for generating high-quality and coherent text, where context and continuity are important. Keeping the number of beams to fewer than 6 helps to avoid a high computational load while still delivering good results. Therefore, we set it to <i>5</i>.<br>\n<br>\n    The <b>BeamSampler</b> is a component commonly found in natural language processing (NLP) libraries, such as those used for text generation with neural networks. It implements a beam search strategy for generating text.<br>\n<br>\n    <b>Beam search</b> is an algorithm used in sequence generation tasks to expand the most promising nodes in a tree and keep a fixed number of these candidates (the \"beam width\") at each step. Unlike greedy search, which only keeps the single best candidate at each step, beam search balances between breadth and depth exploration of the search space, potentially leading to better quality sequences by considering more alternatives at each step.<br>\n<br>\n    When used in text generation, the <b>BeamSampler</b> selects the most likely next tokens at each step of the sequence generation, keeping a set number of these sequences as candidates (defined by the beam width). This process continues until the sequences are completed, usually by reaching a maximum length or an end-of-sequence token. The <b>BeamSampler</b> is particularly useful for tasks requiring high-quality outputs.<br>\n<br>    \n    Before defining helper functions, we import <mark>Pandas</mark> for tabular display of our results, the <mark>re</mark> module for text searching and manipulation using regular expressions, and the <mark>chain</mark> module to flatten an iterable more memory-efficiently and potentially faster.\n</span>","metadata":{}},{"cell_type":"code","source":"%%capture\n\n\nimport pandas as pd\nfrom itertools import chain\nimport re\n\n\n# Format the entire example as a single string.\ntemplate = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\\n\\n\"\n\n\n# CSS to inject for styling pandas dataframes\ndef display_dataframe_custom_style(df, max_width_px=350, min_height_px=50, \n                                   even_row_color=\"#BEFFBF\", odd_row_color=\"#82E183\"):\n    \n    \n    \"\"\"\n    Displays a pandas DataFrame with custom styling in a Jupyter Notebook.\n\n    This function applies a custom CSS style to a pandas DataFrame for display purposes,\n    including setting maximum column width, minimum row height, custom colors for even\n    and odd rows for better readability (zebra striping), and ensuring text wrapping\n    within cells.\n\n    Parameters:\n    - df (pandas.DataFrame): The DataFrame to be displayed.\n    - max_width_px (int, optional): Maximum width in pixels for each column. Defaults to 350.\n    - min_height_px (int, optional): Minimum height in pixels for each row. Defaults to 50.\n    - even_row_color (str, optional): Background color for even rows, specified as a CSS\n      color value. Defaults to \"#BEFFBF\" (light green).\n    - odd_row_color (str, optional): Background color for odd rows, specified as a CSS\n      color value. Defaults to \"#82E183\" (green).\n\n    Returns:\n    - None. The function directly displays the styled DataFrame in a Jupyter Notebook\n      environment using IPython's display() function and HTML rendering.\n\n    Example:\n    >>> import pandas as pd\n    >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    >>> display_dataframe_custom_style(df, max_width_px=200, min_height_px=30,\n                                       even_row_color=\"#F9F9F9\", odd_row_color=\"#EFF7FF\")\n\n    Note:\n    - This function is designed to work in Jupyter Notebook or JupyterLab environments where\n      the IPython display() function and HTML rendering are supported.\n    - Adjusting the CSS color values and dimensions allows for customization according to\n      different presentation needs or visual preferences.\n    \"\"\"\n        \n    css = f\"\"\"\n    <style>\n        .custom_dataframe td, .custom_dataframe th {{\n            white-space: nowrap;\n            max-width: {max_width_px}px; /* Adjust column width */\n            min-height: {min_height_px}px; /* Adjust row height */\n            border: 1px solid black;\n            overflow: hidden;\n            text-overflow: ellipsis;\n            vertical-align: top;\n            text-align: left;\n            border: 1px solid black;\n        }}\n        .custom_dataframe td {{\n            white-space: normal; /* Wrap text */\n        }}\n        /* Zebra striping for rows */\n        .custom_dataframe tr:nth-child(odd) {{\n            background-color: {odd_row_color}; /* Green for odd rows */\n        }}\n        .custom_dataframe tr:nth-child(even) {{\n            background-color: {even_row_color}; /* Light green for even rows */\n        }}\n    </style>\n    \"\"\"\n    display(HTML(css + df.to_html(classes=\"custom_dataframe\")))\n   \n    \ndef create_df_from_dicts(models_dict, max_width_px=350, min_height_px=50, \n                            even_row_color=\"#BEFFBF\", odd_row_color=\"#82E183\"):\n    \n    \"\"\"\n    Converts a nested dictionary structure into a pandas DataFrame and displays it with custom styling.\n    The function is designed for scenarios where model outputs, stored in dictionaries with model names\n    as keys and query-response pairs as values, need to be organized and visually presented. The output\n    DataFrame arranges model responses row-wise per query, facilitating comparison across models.\n\n    Parameters:\n    - models_dict (dict): A nested dictionary with a structure {model_name: {query: response}}. It contains\n      model names as keys and dictionaries of query-response pairs as values, representing the output of\n      different models for a set of queries.\n    - max_width_px (int): Maximum width in pixels for each column in the displayed DataFrame. This controls\n      the column width to ensure content visibility without excessive overflow. Defaults to 350px.\n    - min_height_px (int): Minimum height in pixels for each row in the displayed DataFrame, enhancing row\n      separation and readability. Defaults to 50px.\n    - even_row_color (str): The background color code (CSS-compatible) for even rows, aiding in visual\n      differentiation between successive rows. Defaults to a light green (\"#BEFFBF\").\n    - odd_row_color (str): The background color code (CSS-compatible) for odd rows, complementing the\n      even_row_color to achieve a zebra-striping effect for better readability. Defaults to a green\n      (\"#82E183\").\n\n    Returns:\n    - None: Instead of returning a value, this function directly displays the DataFrame with custom styling\n      within a Jupyter notebook environment. The DataFrame is also explicitly deleted after display to free\n      up memory.\n\n    Requires:\n    - pandas: The DataFrame creation relies on pandas. Ensure pandas is installed (`pip install pandas`) and\n      imported (`import pandas as pd`) in the notebook or script.\n    - IPython.display: For displaying HTML content, ensure IPython's display functionality is available.\n\n    Example:\n    Assuming `models_dict` is a dictionary containing model outputs:\n    >>> models_dict = {\"ModelA\": {\"Query1\": \"Response1\", \"Query2\": \"Response2\"},\n                       \"ModelB\": {\"Query1\": \"Response3\", \"Query2\": \"Response4\"}}\n    >>> create_df_from_dicts(models_dict)\n\n    This will display a DataFrame with queries as rows, models as columns, and their corresponding\n    responses filled in, all with the specified custom styling.\n\n    Note:\n    - The function assumes the presence of `display_dataframe_custom_style` for applying custom styles to\n      the DataFrame. Ensure this function is defined and operational in your working environment.\n    - The function is optimized for Jupyter notebooks where direct DataFrame visualization and custom CSS\n      styling can significantly enhance data presentation.\n    \"\"\"\n    \n    # Create an empty outer dictionary to store model output\n    data = {}\n    \n    # Loop over queries\n    for query in queries:\n        \n        # Create an empty initial inner dictionary to store output's keys and values\n        data[query] = {}\n        \n        # Loop over models to find their output for each query \n        for key, value in models_dict.items(): \n            \n            try:\n                # Create a data dictionary in the format: {query:{model:response}}\n                data[query][key] = value[query]\n                \n            except Exception as e:\n                print(f\"The following key does not exist in the dictionary: {e}\\n\")  \n        \n    # Create DataFrame\n    df = pd.DataFrame.from_dict(data, orient='index')\n    \n    print(\"\\n\")\n    \n    # Display the styled DataFrame\n    display_dataframe_custom_style(df, \n                                   max_width_px=max_width_px, \n                                   min_height_px=min_height_px, \n                                   even_row_color=even_row_color, \n                                   odd_row_color=odd_row_color\n                                  )\n    \n    print(\"\\n\")\n    \n    # Delete the dataframe\n    del df\n    \n    \ndef clean_text(the_list):\n    \n    \"\"\"\n    Cleans a list of text items by removing empty strings and whitespace.\n\n    This function takes a list of strings and performs two main cleaning operations:\n    1. Removes any items that are empty or contain only whitespace characters.\n    2. Removes items that are exactly equal to an empty string ('').\n\n    The function iterates through the provided list, applies these cleaning criteria,\n    and returns a new list containing only the items that do not meet the conditions\n    for removal.\n\n    Parameters:\n    - the_list (list of str): A list of strings that needs to be cleaned. The list\n      may contain empty strings, strings with only whitespace, and regular text strings.\n\n    Returns:\n    - list of str: A cleaned list of strings, with empty and whitespace-only strings removed.\n\n    Example:\n    >>> sample_list = [\"Hello\", \" \", \"\", \"World\", \"  \", \"Python\"]\n    >>> clean_text(sample_list)\n    ['Hello', 'World', 'Python']\n\n    Note:\n    - The function ensures that the output list will not contain any strings that are\n      empty or consist solely of whitespace. However, it does not modify the content\n      of non-empty strings or remove whitespace from within or around non-empty strings.\n    - The function returns a new list and does not modify the original list in-place.\n    \"\"\"\n    \n    # Remove any remaining empty strings\n    stripped_list = [item for item in the_list if item.strip()]  \n\n    # Remove '' characters from the list\n    clean_list = [item for item in stripped_list if item != '']\n    \n    # Delete the previous list\n    del stripped_list\n    \n    return clean_list\n      \n\ndef format_response(response: str) -> str:\n    \n    \"\"\"\n    Cleans and formats a given response string to extract and concatenate specific segments\n    of text located between markers \"Response:\" and the next occurrence of \"Instruction:\".\n\n    The function performs several steps to process the input text:\n    1. Removes ANSI escape sequences used for coloring or styling text in terminals.\n    2. Splits the text into a list of sentences based on newline characters.\n    3. Further cleans the list of sentences (using a presumed existing function `clean_text`).\n    4. Identifies segments of text that start with \"Response:\" and end before the next \"Instruction:\"\n       marker. It assumes that the first \"Instruction:\" marker is not part of any desired text\n       segment and therefore skips it.\n    5. Concatenates these text segments into a single string.\n\n    Parameters:\n    - response (str): The original text string containing the response to be cleaned and formatted.\n\n    Returns:\n    - str: A cleaned and concatenated string containing the specific segments of text extracted\n           from the original response.\n\n    Note:\n    - This function assumes the presence of \"Response:\" and \"Instruction:\" markers in the text\n      to identify relevant segments. The functionality depends on the structured format of the\n      input text.\n    - The `clean_text` function used within this code is presumed to exist elsewhere in the codebase.\n      Its purpose is to perform additional cleaning on the list of sentences.\n    - The ANSI escape sequences are removed using a regular expression that matches these sequences.\n    - A large arbitrary number is appended to the list of instruction indices as a workaround to\n      ensure the last segment of text is correctly identified and included.\n\n    Example:\n    Assuming an input string `response` structured with \"Response:\" and \"Instruction:\" markers,\n    this function will return a single string containing the concatenated text of interest.\n    \n    >>> response = \"Instruction: Do X\\\\nResponse: Result of X\\\\nInstruction: Do Y\"\n    >>> print(format_response(response))\n    Result of X\n\n    This function is tailored for a specific text format and might require adjustments for\n    different formats or requirements.\n    \"\"\"\n\n    # Remove special characters using regex\n    cleaned_text = re.sub(r'\\x1b\\[\\d+m', '', response)\n\n    # Arrange the text in a list of sentences\n    sentences = cleaned_text.split(\"\\n\")\n    sentences = clean_text(sentences)\n    \n    # Create an empty list to store response text only\n    all_responses = []\n    \n    # Find the indices of the word \"Instruction\" in the list, and store them in a separate list\n    instruction_list = [index for index, value in enumerate(sentences) if value == \"Instruction:\"]\n    \n    # Delete the first item in the list...\n    del instruction_list[0]\n    \n    # And append a large number to the list. This way, we can match the index of the first\n    # \"Response\" term with the index of the second \"Instruction\" term (the first one was \n    # removed before), so we can extract the text between them. This will be response text\n    # as such 1st Response <--response_text--> 2nd Instruction.\n    instruction_list.append(100_000)\n    \n    # Find the indices of the word \"Instruction\" in the list, and store them in a separate list\n    response_list = [index for index, value in enumerate(sentences) if value == \"Response:\"]\n    \n    # Zip two lists to fulfill matching as described above\n    index_list = list(zip(response_list, instruction_list))\n    \n    # Loop over the list of tuples to extract the response text as described above\n    for index_pair in index_list:\n        \n        # Segment is the response text only\n        segment = sentences[index_pair[0] + 1: index_pair[1]]\n\n        # So each response is added to the new list. This will create a list of lists.\n        all_responses.append(segment)\n\n    # Flatten the nested list to a format like [1, 2, 3, 4, 5, 6]\n    final_list = list(chain.from_iterable(all_responses))\n\n    # Convert the list to string\n    paragraph = \" \".join(final_list)\n\n    return paragraph\n\n\ndef define_sampler(num_beams):\n    \n    \"\"\"\n    Initializes and returns a BeamSampler object with the specified number of beams.\n\n    This function creates a BeamSampler instance from the keras_nlp library. The BeamSampler\n    is used in natural language processing tasks to generate text predictions. It explores multiple\n    paths or 'beams' through the model's prediction space and selects the most probable sequence of\n    tokens as its output. This approach can improve the quality of the generated text by considering\n    a broader range of possibilities.\n\n    Parameters:\n    - num_beams (int): The number of beams to be used in the BeamSampler. This value determines how\n      many different paths through the prediction space the sampler will consider. A higher number of\n      beams allows for a broader search, which can lead to better results but requires more computational\n      resources.\n\n    Returns:\n    - keras_nlp.samplers.BeamSampler: An instance of the BeamSampler object configured with the specified\n      number of beams and set to return only the most probable sequence by default.\n\n    Example:\n    >>> sampler = define_sampler(5)\n    This initializes a BeamSampler object that will consider 5 different paths during text generation.\n\n    Note:\n    - The `return_all_beams` parameter of the BeamSampler is fixed to `False` in this function, meaning\n      that only the most probable sequence will be returned during text generation. If you need to return\n      all beams (i.e., all considered sequences), you would need to modify the function or create the\n      BeamSampler object directly with `return_all_beams=True`.\n    - Ensure that `keras_nlp` is installed (`pip install keras_nlp`) and imported before calling this\n      function.\n    \"\"\"\n    \n    sampler = keras_nlp.samplers.BeamSampler(num_beams=num_beams, return_all_beams=False)\n    \n    return sampler\n\n\ndef generate_prompt(input_text):\n    \n    \"\"\"\n    Generates a formatted prompt based on a predefined template and input text.\n\n    This function takes an input text, which is intended to serve as an instruction or\n    a basis for generating a prompt, and formats it using a predefined template. The template\n    is expected to have placeholders for 'instruction' and potentially for 'response', though\n    the 'response' part is left empty in this function. The result is a prompt ready for\n    further processing or for being fed into a model for generating responses.\n    \n\n    Parameters:\n    - input_text (str): The text to be inserted into the prompt template as an instruction.\n\n    Returns:\n    - str: A prompt string formatted according to the predefined template, with the instruction\n           part filled in with `input_text` and the response part left empty.\n\n    Example:\n    Assuming a predefined template string:\n    >>> template = \"Instruction: {instruction}\\\\nResponse: {response}\"\n    >>> print(generate_prompt(\"Translate 'Hello, world' into Spanish.\"))\n    Instruction: Translate 'Hello, world' into Spanish.\n    Response: \n\n    Note:\n    - The `template` variable used for formatting the prompt should be defined outside this\n      function and must include at least the 'instruction' placeholder. This example assumes\n      the template also includes a 'response' placeholder, which remains empty in the output.\n    - This function is designed to be flexible, accommodating any template structure as long\n      as it follows the placeholder requirements.\n    \"\"\"\n    \n    prompt = template.format(\n        instruction = input_text,\n        response = \"\",\n    )\n    \n    return prompt\n\n\ndef generate_text_keras(input_text, model, printing=True):\n    \n    \"\"\"\n    Generates text based on an input prompt using a compiled Keras model and a custom sampler.\n\n    This function takes an input text to form a prompt using a predefined template, compiles\n    the provided Keras model with a custom sampler for text generation, and then generates a\n    response based on the compiled model and prompt. The generated text can optionally be\n    printed to the console.\n\n    Parameters:\n    - input_text (str): The input text that forms the basis of the prompt for text generation.\n    - model (keras.Model): The Keras model to be used for generating text. This model should\n      support the `.generate()` method for text generation after being compiled with a custom\n      sampler.\n    - printing (bool, optional): If True, the generated text will be printed to the console.\n      Defaults to True.\n\n    Returns:\n    - str: The text generated by the model based on the input prompt.\n\n    Example:\n    Assuming the existence of a Keras model `my_model` capable of text generation, and the\n    predefined functions `generate_prompt` for prompt generation and `define_sampler` for\n    creating a sampler:\n    \n    >>> response = generate_text_keras(\"Translate 'Hello, world' into French.\", my_model)\n    [Generated response printed here]\n    \n    Note:\n    - This function relies on the `generate_prompt` function to create the prompt and the\n      `define_sampler` function to define the sampling strategy. Ensure these functions are\n      correctly defined and accessible.\n    - `NUM_BEAMS` and `OUTPUT_MAX_LEN` are constants that should be defined elsewhere in the\n      code, specifying the number of beams for the sampling strategy and the maximum length\n      of the output text, respectively.\n    - The model is expected to have been trained or configured to generate text based on the\n      type of prompts produced by `generate_prompt`.\n    \"\"\"\n    \n    # Generate the prompt\n    prompt = generate_prompt(input_text)\n    \n    # Define the sampler\n    sampler = define_sampler(NUM_BEAMS)\n    \n    # Compile the generate() function with a custom sampler\n    model.compile(sampler=sampler)\n        \n    # Generate the response using the model\n    response = model.generate(prompt, max_length=OUTPUT_MAX_LEN)\n\n    # Print the response\n    if printing:\n        print(response, \"\\n\")\n\n    return response\n\n\ndef generate_and_format_response(queries, responses_dict, model, model_key, printing=True):\n    \n    \"\"\"\n    Generates and formats responses for a given list of queries using a specified model, and then\n    aggregates these responses in a dictionary. This dictionary is keyed by the queries and is added\n    to a master dictionary under a given model key. This function is particularly useful for organizing\n    generated text responses from different models for comparison or further processing.\n\n    Parameters:\n    - queries (list of str): A list of string queries for which the model generates responses. Each query\n      is processed individually by the model.\n    - responses_dict (dict): A dictionary where generated and formatted responses will be stored. The keys\n      are the queries, and the values are their corresponding formatted responses.\n    - model: The model used for generating responses. This model should have a text generation capability,\n      typically a Keras model pre-configured or trained for natural language generation tasks.\n    - model_key (str): A unique string identifier for the model. This key is used to store the aggregated\n      responses in the master dictionary, allowing for organized access based on the model used.\n    - printing (bool, optional): A flag determining whether to print the generated responses to the console.\n      Useful for debugging or interactive exploration. Defaults to True.\n\n    Returns:\n    - dict: The updated master dictionary (referred to as `MODELS_DICT` within the function) containing\n      the responses aggregated under the specified `model_key`.\n\n    Raises:\n    - This function implicitly depends on the successful execution of `generate_text_keras` for generating\n      text responses and `format_response` for formatting these responses. Any exceptions raised by these\n      functions must be handled appropriately within those functions.\n\n    Example:\n    >>> queries = [\"What is AI?\", \"Explain machine learning\"]\n    >>> responses_dict = {}\n    >>> model = my_text_generation_model\n    >>> model_key = \"MyModel\"\n    >>> updated_dict = generate_and_format_response(queries, responses_dict, model, model_key)\n    \n    Now `updated_dict` contains the formatted responses from `my_text_generation_model` under \"MyModel\" key.\n\n    Note:\n    - `MODELS_DICT` is a global dictionary expected to exist outside this function where the responses are aggregated.\n      Ensure that this dictionary is properly initialized before calling this function.\n    - The function `generate_text_keras` is used for generating responses from the model, and `format_response`\n      is used for formatting the generated text. Ensure these functions are defined and accessible in your\n      codebase, and that they are compatible with the model and response structure being used.\n    \"\"\"\n        \n    for query in queries:\n    \n        # Generate responses for queries given\n        response = generate_text_keras(query, model, printing=printing)\n\n        # Format the response\n        formatted_response = format_response(response)\n\n        # Store queries and formatted responses\n        responses_dict[query] = formatted_response\n    \n    # Add the dictionary to the general collection of all models' outputs\n    MODELS_DICT[model_key] = responses_dict\n    \n    return MODELS_DICT","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:12:22.109741Z","iopub.execute_input":"2024-04-09T05:12:22.110164Z","iopub.status.idle":"2024-04-09T05:12:23.178882Z","shell.execute_reply.started":"2024-04-09T05:12:22.110125Z","shell.execute_reply":"2024-04-09T05:12:23.177570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<div class=\"subsection_title\">\n    <a href=\"#table_of_contents\" style=\"color: #46A050;\" >2.2. Pretrained Gemma Model from Keras Library</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nWe import the <b>os</b> module to set some environment variables.<br>\n<br>    \nThe first line below sets the <mark>KERAS_BACKEND</mark> environment variable to <b>\"jax\"</b>. This action configures <b>Keras</b>, a high-level neural networks API, to use <b>JAX</b> as its backend for computations. <b>JAX</b> is a library for high-performance numerical computing, particularly suited for machine learning and deep learning tasks. By setting this environment variable, any subsequent operations performed using </b>Keras</b> in this environment will leverage <b>JAX</b> for tensor operations, differentiation, and optimization tasks,<br>\n<br>\nThe second line sets the <mark>XLA_PYTHON_CLIENT_MEM_FRACTION</mark> environment variable to <i>1.00</i>. This configuration is specific to the <b>XLA (Accelerated Linear Algebra)</b> compiler, which is used by libraries such as JAX and TensorFlow for optimizing computations for speed and efficiency on hardware accelerators like GPUs and TPUs. By setting <mark>XLA_PYTHON_CLIENT_MEM_FRACTION</mark> to <i>1.00</i>, it instructs the XLA Python client to allocate 100% of the available memory on the GPU (or TPU) for its computations. This is aimed at avoiding memory fragmentation, ensuring that XLA has access to the largest possible contiguous block of memory. Memory fragmentation can lead to inefficiencies and errors when there isn't enough contiguous memory to perform operations, so this setting can help mitigate such issues, especially in environments with high memory demand.\n</span>","metadata":{}},{"cell_type":"code","source":"%%capture\n\n\nimport os\n\n\n# Configure the backend for JAX\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\n# Avoid memory fragmentation\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:12:23.181142Z","iopub.execute_input":"2024-04-09T05:12:23.181701Z","iopub.status.idle":"2024-04-09T05:12:23.188058Z","shell.execute_reply.started":"2024-04-09T05:12:23.181661Z","shell.execute_reply":"2024-04-09T05:12:23.186870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nWe are loading the untuned version of the Gemma model from the Keras library.<br>\nSee: <a href=\"https://keras.io/api/keras_nlp/models/gemma/gemma_causal_lm/\">GemmaCausalLM model</a>\n</span>","metadata":{}},{"cell_type":"code","source":"%%capture\n\n\nimport keras\nimport keras_nlp\n\n\ngemma_llm = keras_nlp.models.GemmaCausalLM.from_preset(SELECTED_MODEL)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-09T05:12:23.189705Z","iopub.execute_input":"2024-04-09T05:12:23.190401Z","iopub.status.idle":"2024-04-09T05:13:37.889212Z","shell.execute_reply.started":"2024-04-09T05:12:23.190363Z","shell.execute_reply":"2024-04-09T05:13:37.888345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nResponse generation:\n</span>","metadata":{}},{"cell_type":"code","source":" # Create an empty dictionary to store queries and responses\npretrained_model_dict = {}\n\n# Generate a response to each query, using the model\nMODELS_DICT = generate_and_format_response(queries, \n                                            pretrained_model_dict, \n                                            gemma_llm, \n                                            \"Pretrained_Model\", \n                                            printing=False\n                                          )","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:13:37.890487Z","iopub.execute_input":"2024-04-09T05:13:37.891062Z","iopub.status.idle":"2024-04-09T05:16:00.413333Z","shell.execute_reply.started":"2024-04-09T05:13:37.891033Z","shell.execute_reply":"2024-04-09T05:16:00.412159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nDisplay the results in an organized and comparable manner through Pandas dataframe.\n</span>","metadata":{}},{"cell_type":"code","source":"# Display the cumulative list of models and their outputs\ncreate_df_from_dicts(\n                    MODELS_DICT, \n                    max_width_px=800, \n                    min_height_px=50, \n                    even_row_color=\"#BEFFBF\", \n                    odd_row_color=\"#82E183\"\n                )","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:16:00.415030Z","iopub.execute_input":"2024-04-09T05:16:00.415518Z","iopub.status.idle":"2024-04-09T05:16:00.434342Z","shell.execute_reply.started":"2024-04-09T05:16:00.415468Z","shell.execute_reply":"2024-04-09T05:16:00.433162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nAs observed in the table above, the <b>Pretrained Model</b> tends to provide lengthy answers, often with repetitions of the same text and/or incomplete sentences. It may also pose additional questions and answer them. Moreover, responses may include statements that are relatively irrelevant. Let's see if we can improve this by fine-tuning.\n</span>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n<div class=\"section_title\">\n    <a href=\"#table_of_contents\" style=\"color: white;\" >3. Fine-tuning Gemma</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<center><img src=\"https://i.ibb.co/F4GVhTY/gemma-keras-finetuning.png\" alt=\"gemma-keras-finetuning\" border=\"0\" width=420></center>\n<center>Fine-tuning Gemma (Image by Author)</center>\n<br><br>\n<span style=\"font-size:medium;\">\n    Among the two Gemma models mentioned earlier (2B and 7B), the <b>instruction-tuned</b> version was trained with human language interactions, while the <b>pretrained</b> version was not specifically trained on tasks or instructions beyond the Gemma core data training set. Given our objective of explaining data science concepts, we should opt for the pretrained version and fine-tune it with our specialized dataset, which includes data science concepts, questions, and their corresponding answers. For this purpose, we'll select the 2B version of the pretrained model and preserve the refined model for further use later in the notebook.\n</span>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n<div class=\"subsection_title\">\n    <a href=\"#table_of_contents\" style=\"color: #46A050;\" >3.1. Preparation for Fine-tuning</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nDefine parameters:\n</span>","metadata":{}},{"cell_type":"code","source":"# Path to our dataset\nPATH = \"/kaggle/input/1000-data-science-concepts/\"\n\n# Hyperparameters\nLORA_RANK = 8\nINPUT_MAX_LEN = 128\nLEARNING_RATE = 5e-5\nWEIGHT_DECAY = 0.01\nEPOCHS = 3\nBATCH_SIZE = 1","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:16:00.435959Z","iopub.execute_input":"2024-04-09T05:16:00.436305Z","iopub.status.idle":"2024-04-09T05:16:00.442200Z","shell.execute_reply.started":"2024-04-09T05:16:00.436276Z","shell.execute_reply":"2024-04-09T05:16:00.441006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\n    We will explain these parameters in detail when we reach the point where they are used.\n</span>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n<div class=\"subsection_title\">\n    <a href=\"#table_of_contents\" style=\"color: #46A050;\" >3.2. Data</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">    \nTo fine-tune our model, we will use the <a href=\"https://www.kaggle.com/datasets/hserdaraltan/1000-data-science-concepts\">1000+ Data Science Concepts</a> dataset, which was specifically created for use with the computational notebook associated with this analysis. We can access the dataset via the <mark>PATH</mark> parameter defined above. The dataset's metadata and methodology have been described in detail on its accompanying webpage. Let's import the data and display it:\n</span>","metadata":{}},{"cell_type":"code","source":"# Dataset\ndf_ds_concepts = pd.read_csv(PATH + \"data_science_concepts.csv\")\n\n# Display the df\ndisplay_dataframe_custom_style(\n                            df_ds_concepts.head(10), \n                            max_width_px=500, \n                            min_height_px=50, \n                            even_row_color=\"#BEFFBF\", \n                            odd_row_color=\"#82E183\"\n                        )","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:16:00.443685Z","iopub.execute_input":"2024-04-09T05:16:00.444037Z","iopub.status.idle":"2024-04-09T05:16:00.493688Z","shell.execute_reply.started":"2024-04-09T05:16:00.444001Z","shell.execute_reply":"2024-04-09T05:16:00.492626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"text-with-green-strip\">\n<p style=\"font-size: 14px; margin-left: 70px;\">\n    The above code snippet performs the following operations:<br>\n    <b>Reads a CSV File into a DataFrame</b>: It loads a CSV file named <span class=\"marks\">data_science_concepts.csv</span> from a specified path (<span class=\"marks\">PATH</span>) into a pandas DataFrame called <span class=\"marks\">df_ds_concepts</span>. This is achieved using the <span class=\"marks\">pd.read_csv()</span> function from the pandas library, which is designed to read data from a comma-separated values (CSV) file into a pandas DataFrame.<br>\n    <b>Displays a Styled Subset of the DataFrame</b>: It then displays the first 10 rows of this DataFrame with custom styling applied, using the <span class=\"marks\">display_dataframe_custom_style</span> function. The customization includes:<br>\n</p>\n<ul class=\"custom-ul\">\n    <li>Setting the maximum column width to 1000 pixels (<span class=\"marks\">max_width_px=1000</span>), which adjusts how much of the text in each cell can be shown before it gets truncated or wrapped.</li>\n    <li>Setting the minimum height of each row to 50 pixels (<span class=\"marks\">min_height_px=50</span>), affecting the vertical spacing of the table.</li>\n    <li>Applying different background colors for even and odd rows to improve readability, with even rows colored light green (<i>#BEFFBF</i>) and odd rows colored green (<i>#82E183</i>).</li>\n</ul>\n<p style=\"font-size: 14px; margin-left: 70px;\">\n    This approach of displaying the DataFrame not only helps in presenting the data in a more organized and visually appealing manner but also aids in quickly assessing the data's structure and initial rows for any immediate insights or validations needed.\n</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nWe have a dataset consisting of 1,070 records, organized into two columns. The first column contains questions about data science concepts or terms, and the second column provides the answers. We need to convert our dataframe to a format suitable to train on Gemma.\n</span>","metadata":{}},{"cell_type":"code","source":"# Initialize an empty list to store the instructions and responses formatted for training\ntraining_data = []\n\n# Iterate over each row of the dataframe\nfor index, row in df_ds_concepts.iterrows():\n    \n    # Extract the instruction\n    instruction_string = row[\"Question\"]\n    \n    # Extract the response\n    response_string = row[\"Answer\"]\n    \n    # Create the formatted string\n    prompt_string = f'Instruction:\\n{instruction_string}\\n\\nResponse:\\n{response_string}'\n    \n    # Append the formatted string to the list\n    training_data.append(prompt_string)\n\n# Print the training data\nfor item in training_data[:3]:\n    print(item, \"\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:16:00.495379Z","iopub.execute_input":"2024-04-09T05:16:00.495814Z","iopub.status.idle":"2024-04-09T05:16:00.578506Z","shell.execute_reply.started":"2024-04-09T05:16:00.495775Z","shell.execute_reply":"2024-04-09T05:16:00.577458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"text-with-green-strip\">\n    <p style=\"font-size: 14px; margin-left: 70px;\">\n        The code snippet processes a DataFrame <span class=\"marks\">df_ds_concepts</span>, which contains data science concepts in a question-and-answer format, and prepares it for use as training data by performing the following operations:<br>\n        <b>Initialize an Empty List for Training Data</b>: An empty list named <span class=\"marks\">training_data</span> is created to hold formatted training data that will be generated from the DataFrame.<br>\n        <b>Iterate Over DataFrame Rows</b>: The code loops through each row of the <span class=\"marks\">df_ds_concepts</span> DataFrame, extracting \"Question\" and \"Answer\" columns to form instruction and response strings.<br>\n        <b>Format and Append Training Data</b>: Each question-answer pair is formatted into a single string with specified labels (\"Instruction:\" and \"Response:\") and appended to the <span class=\"marks\">training_data</span> list, creating a collection of formatted strings for training.<br>\n        <b>Print Sample Training Data</b>: The first three formatted strings from the <span class=\"marks\">training_data</span> list are printed to the console for review, demonstrating the data preparation output.<br>\n    </p>\n    <p style=\"font-size: 14px; margin-left: 70px;\">\n        This methodology is tailored for organizing text data for natural language processing (NLP) tasks, particularly for training machine learning models to understand or generate responses based on given instructions or queries.\n    </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nQueries we have mentioned at the beginning of the notebook are purposely designed, and not all of them find answers in the data. The below table shows for which query is possible to find some answer in the data.\n</span>","metadata":{}},{"cell_type":"markdown","source":"<center><img src=\"https://i.ibb.co/26gFzZ8/train-data-3.png\" alt=\"gemma-keras-finetuning\" border=\"0\" width=800></center>\n<center>Answers Covered in the Training Data (Image by Author)</center>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\n<i>Query_3</i> is partially answered by the data. The concept of one-shot learning, however, is not explained in the dataset. <i>Query_7</i>, although nonsensical, aims to explore whether the words 'frequency' and 'tune' could trigger a relevant association or response. <i>Query_7</i> is an English back-translation of a quote from an episode of 'Crime Story,' in which the late Dennis Farina starred. The quote is from a scene set in the 1960s, involving a dramatic and intense moment in a hairdresser's shop. This quote has now been used as a query for Gemma!\n</span>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"8\"></a>\n<div class=\"subsection_title\">\n    <a href=\"#table_of_contents\" style=\"color: #46A050;\" >3.3. Comparable Architectures of the Model</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nWe have previously loaded the pretrained version. Let's examine its architecture:\n</span>","metadata":{}},{"cell_type":"code","source":"gemma_llm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:16:00.584242Z","iopub.execute_input":"2024-04-09T05:16:00.584553Z","iopub.status.idle":"2024-04-09T05:16:00.616159Z","shell.execute_reply.started":"2024-04-09T05:16:00.584525Z","shell.execute_reply":"2024-04-09T05:16:00.615195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<pre><code>gemma_llm = keras_nlp.models.GemmaCausalLM.<mark>from_preset</mark>(SELECTED_MODEL)</code></pre><br>\n<br>\n<span style=\"font-size:medium;\">\nThe <mark>from_preset</mark> method instantiates the model from a preset architecture and weights. In the code above, the string \"<b>gemma_2b_en</b>\" specifies the preset architecture — a Gemma model with 2.5 billion parameters. They are too many to train! We need to reduce the trainable parameters during fine-tuning to save memory space and increase speed.<br>\n<br>\n    The <b>GemmaBackbone</b> model represents the core network of Gemma, equipped with hyperparameters. This backbone encapsulates the foundational Transformer network architecture used in the Gemma model, incorporating both the embedding lookups and Transformer layers. It is designed to output the final hidden states for each token rather than generating predictions over the vocabulary. For tasks requiring text generation, we will proceed to fine-tune the model on a higher-level basis.<br>\nSource: <a href=\"https://keras.io/api/keras_nlp/models/gemma/gemma_backbone/\">GemmaBackbone model</a><br>\n<br>\nFine-tuning massive language models can be extremely costly, requiring significant hardware resources and incurring substantial costs for storage and managing separate instances for various tasks. <b>Low-Rank Adaptation (LoRA)</b> presents an effective method of adaptation that does not add to inference time or shorten the length of input sequences, all while preserving the quality of the model. It significantly decreases the number of parameters that need to be trained for subsequent tasks. Crucially, it supports rapid switching between tasks when offered as a service, thanks to its ability to share most of the model's parameters.<br>\nSource: <a href=\"https://arxiv.org/abs/2106.09685\">LoRA: Low-Rank Adaptation of Large Language Models</a><br>\n<br>\n<b>LoRA</b> is a parameter-efficient fine-tuning technique designed for large language models (LLMs) that significantly reduces the number of trainable parameters for downstream tasks. It focuses on fine-tuning only a fraction of the model's total parameters by freezing the original model weights and introducing adapter layers. These layers are decomposed into low-rank matrices, effectively minimizing the training overhead. <b>LoRA</b> maintains the performance quality comparable to fully fine-tuned models without adding any inference latency, as the adapter weights can be merged with the base model. This approach not only accelerates the training process and enhances memory efficiency but also results in smaller model sizes (a few hundred MBs), ensuring high-quality model outputs with a more manageable footprint. Therefore, we will use <b>LoRA</b> in our fine-tuning.<br>\nSources:<br>\n<a href=\"https://huggingface.co/blog/gemma-peft\">Fine-Tuning Gemma Models in Hugging Face</a><br>\n<a href=\"https://pytorch.org/blog/finetune-llms/\">Finetune LLMs on your own consumer hardware using tools from PyTorch and Hugging Face ecosystem</a><br>    \n<a href=\"https://ai.google.dev/gemma/docs/lora_tuning\">Fine-tune Gemma models in Keras using LoRA</a><br>\n<br>\nA higher <b>LoRA</b> rank allows for more detailed changes, albeit at the cost of increasing the number of trainable parameters. Conversely, a lower rank reduces computational overhead while possibly limiting the precision of adaptation. Since we will be using limited data to fine-tune our model, to avoid overfitting, we opt for a lower rank, such as <i>8</i>. The <b>LoRA</b> paper also suggests that a lower rank could be sufficiently good to produce quality results.<br>\nSource: <a href=\"https://arxiv.org/abs/2106.09685\">LoRA: Low-Rank Adaptation of Large Language Models</a><br>\n</span>","metadata":{}},{"cell_type":"code","source":"gemma_llm.backbone.enable_lora(rank=LORA_RANK)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:16:00.617371Z","iopub.execute_input":"2024-04-09T05:16:00.617711Z","iopub.status.idle":"2024-04-09T05:16:01.057157Z","shell.execute_reply.started":"2024-04-09T05:16:00.617684Z","shell.execute_reply":"2024-04-09T05:16:01.056266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nLet' see the new architecture:<br> \n</span>","metadata":{}},{"cell_type":"code","source":"gemma_llm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:16:01.058455Z","iopub.execute_input":"2024-04-09T05:16:01.058810Z","iopub.status.idle":"2024-04-09T05:16:01.090443Z","shell.execute_reply.started":"2024-04-09T05:16:01.058781Z","shell.execute_reply":"2024-04-09T05:16:01.089580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nThe number of trainable parameters was reduced from 2.5 billion to around 2.7 million!\n</span>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"9\"></a>\n<div class=\"subsection_title\">\n    <a href=\"#table_of_contents\" style=\"color: #46A050;\" >3.4. Fine-tuning</a>\n</div>","metadata":{}},{"cell_type":"code","source":"# Our model's name\nmodel_name = \"finetuned_\" + SELECTED_MODEL + \"_for_data_science\"\nprint(\"Name of the model: \", model_name)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:16:01.091526Z","iopub.execute_input":"2024-04-09T05:16:01.091789Z","iopub.status.idle":"2024-04-09T05:16:01.096913Z","shell.execute_reply.started":"2024-04-09T05:16:01.091765Z","shell.execute_reply":"2024-04-09T05:16:01.096017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nThe data and the model are ready for fine-tuning. If we intend to utilize a previously saved model, we must first load it. Otherwise, we may continue to train the model that we have already loaded.<br>\n<br>\nChoosing <b>AdamW</b> for training large language models (LLMs) offers advantages such as better handling of weight decay, leading to improved generalization. Unlike traditional <b>Adam</b>, which applies weight decay before updating the parameters, <b>AdamW</b> decouples weight decay from the gradient updates. In <b>AdamW</b>, weight decay is applied directly to the weights themselves rather than as part of the gradient. This approach decouples weight decay from the learning rate, allowing for more straightforward optimization of the regularization term. Weight decay in <b>AdamW</b> modifies the weights after they are updated during the optimization process, ensuring the regularization effect directly influences the weight values, leading to potentially better generalization in trained models. This distinction can be particularly beneficial in training complex models like LLMs, where fine-tuning the balance between learning and regularization is crucial for achieving optimal performance.<br>\nSource: <a href=\"https://towardsdatascience.com/why-adamw-matters-736223f31b5d\">Why AdamW matters</a><br>\n<br>\n    Excluding <b>layer normalization (\"scale\")</b> and <b>bias</b> terms from weight decay when using <b>AdamW</b> is a common practice because these parameters behave differently from the main weights during training. Weight decay is applied to regularize and prevent overfitting by shrinking weights, but applying it to layer normalization parameters and biases can negatively affect the learning process. \"The purpose of L2 regularization is to spread out the weights in dot products, ensuring that more \"independent measurements\" (dimensions of the input) get used more equally, instead of any one feature dominating the computation. This only makes sense for matrix multiply layers, which the embeddings and the layernorm parameters are not.\"<br>\nSource: <a href=\"https://github.com/karpathy/minGPT/pull/24#issuecomment-679316025\">Weight decay exclusions</a><br>\n<br>\nWe set <mark>INPUT_MAX_LEN</mark> to <i>128</i> due to encountering memory issues with a larger number of token sizes. For the same reason, we maintained <mark>BATCH_SIZE</mark> at <i>1</i>. To avoid increasing the runtime length, we limited the number of <mark>EPOCH</mark> runs to <i>3</i>. This should be enough to determine if there is any improvement in accuracy.\n</span>","metadata":{}},{"cell_type":"code","source":"# Limit the input sequence length to 128 to control memory usage\ngemma_llm.preprocessor.sequence_length = INPUT_MAX_LEN\n\n# Use AdamW (a common optimizer for transformer models)\noptimizer = keras.optimizers.AdamW(\n    learning_rate=LEARNING_RATE,\n    weight_decay=WEIGHT_DECAY,\n)\n\n# Exclude layernorm and bias terms from decay\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\ngemma_llm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\n# Finetune\ngemma_llm.fit(training_data, epochs=EPOCHS, batch_size=BATCH_SIZE)\n\n# Save the model\n# path_to_save_location = \"/kaggle/working/\" + model_name + \".keras\"\n#     gemma_llm.save(path_to_save_location) ","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:16:01.098265Z","iopub.execute_input":"2024-04-09T05:16:01.098608Z","iopub.status.idle":"2024-04-09T05:20:24.779156Z","shell.execute_reply.started":"2024-04-09T05:16:01.098582Z","shell.execute_reply":"2024-04-09T05:20:24.778147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nThe updated architecture of the fine-tuned model:\n</span>","metadata":{}},{"cell_type":"code","source":"gemma_llm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:20:24.780688Z","iopub.execute_input":"2024-04-09T05:20:24.781175Z","iopub.status.idle":"2024-04-09T05:20:24.817029Z","shell.execute_reply.started":"2024-04-09T05:20:24.781137Z","shell.execute_reply":"2024-04-09T05:20:24.816075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\n\"Optimizer params\" in the summary table refers to the number of parameters that the optimizer is actively updating during training. After applying LoRA, this count reflects both the original trainable model parameters and the additional parameters introduced by LoRA.<br>\n<br>\nNow, we can try our prompts to see how the fine-tuned model performs.\n</span>","metadata":{}},{"cell_type":"code","source":"# Create an empty dictionary to store queries and responses\ntrained_model_dict = {}\n\n# Generate a response to each query, using the model\nMODELS_DICT = generate_and_format_response(queries, \n                                            trained_model_dict, \n                                            gemma_llm, \n                                            \"Finetuned_Model\", \n                                            printing=False\n                                          )","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:20:24.818309Z","iopub.execute_input":"2024-04-09T05:20:24.818597Z","iopub.status.idle":"2024-04-09T05:22:20.134756Z","shell.execute_reply.started":"2024-04-09T05:20:24.818572Z","shell.execute_reply":"2024-04-09T05:22:20.133591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the cumulative list of models and their outputs\ncreate_df_from_dicts(\n                    MODELS_DICT, \n                    max_width_px=500, \n                    min_height_px=50, \n                    even_row_color=\"#BEFFBF\", \n                    odd_row_color=\"#82E183\"\n                )","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:22:20.137785Z","iopub.execute_input":"2024-04-09T05:22:20.138254Z","iopub.status.idle":"2024-04-09T05:22:20.150022Z","shell.execute_reply.started":"2024-04-09T05:22:20.138211Z","shell.execute_reply":"2024-04-09T05:22:20.149023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\n    It is evident that the <b>Fine-tuned Model</b> trained on our specific dataset yields pretty accurate results. It delivers concise and pertinent answers. While this is beneficial, there is still a room for improvement.<br>\n<br>\nIt is unlikely that the current data in our data science terminology dataset will become obsolete, as terms tend to remain relevant for extended periods. However, it is inevitable that new terms and concepts will emerge, expanding human knowledge. Our model, being a typical Large Language Model (LLM), is trained on the dataset as of today, making it incapable of accessing the most current information. For example, suppose we trained our model three years ago; it would not be aware of knowledge that has emerged during these three years. To address this limitation, we will install <b>Retrieval-Augmented Generation (RAG)</b> and introduce relevant Wikipedia documents as sources where the model can find direct answers. We will accomplish this using <b>LlamaIndex</b> in the next section. It is important to note that using gigabytes or even more voluminous web data exceeds the scope of this notebook. As a toy example to showcase how we can integrate the RAG capability with the trained <b>Gemma</b> version, we will use only Wikipedia pages. Further descriptions of <b>RAG</b> and <b>LlamaIndex</b> are provided in the next section.\n</span>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"10\"></a>\n<div class=\"section_title\">\n    <a href=\"#table_of_contents\" style=\"color: white;\" >4. RAG with LlamaIndex</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<center><img src=\"https://i.ibb.co/7NDzbb4/gemma-hf-llama-pinecone.png\" alt=\"gemma-langchain\" border=\"0\" width=550></center>\n<center>Gemma, Hugging Face, LlamaIndex, and Pinecone (Image by Author)</center>\n<br><br>\n<span style=\"font-size:medium;\">\n<b>Retrieval Augmented Generation (RAG)</b> is a method that utilizes large language models (LLMs) to search through both structured and unstructured documents. It operates in two main stages: indexing and retrieval, and then generation. During the indexing stage, documents are divided into segments, and the embeddings of these segments are saved in a vector database. In the retrieval and generation stage, segments relevant to the user's query are fetched from the vector database. These segments are then used to guide the LLM, enabling it to generate responses that consider the context provided by the retrieved segments. This technique enhances the model's ability to deliver precise and contextually relevant answers.<br>\nSource: <a href=\"https://netraneupane.medium.com/retrieval-augmented-generation-rag-using-llamaindex-and-mistral-7b-228f93ba670f\">Retrieval Augmented Generation(RAG) using LlamaIndex and Mistral— 7B</a><br>\n<br>\n<b>LlamaIndex</b> is a sophisticated data framework designed to enhance Large Language Model (LLM)-based applications through context augmentation. It offers crucial abstractions that simplify the process of ingesting, organizing, and accessing private or domain-specific data. This framework enables the seamless integration of specific datasets into LLMs, ensuring secure and reliable context injection for improved accuracy in text generation.<br>\nSource: <a href=\"https://docs.llamaindex.ai/en/stable/\">Welcome to LlamaIndex</a><br>\n<br>\n<b>LlamaIndex</b> serves as an accessible and adaptable data framework that links data sources to your large language models (LLMs) in a straightforward manner. It begins by taking the input data you supply and constructing an index from it. This index is then utilized to assist in answering queries pertaining to the input data. Depending on the specific requirements of the task, <b>LlamaIndex</b> is capable of creating various kinds of indexes, including vector indexes, tree indexes, list indexes, and keyword indexes.<br>\n<br>\n</span>","metadata":{}},{"cell_type":"markdown","source":"<center><img src=\"https://i.ibb.co/hdZfN5S/llamaindex-rag-overview.png\" border=\"0\" width=700></center>\n<center>Evaluate RAG with LlamaIndex (Image by OpenAI Cookbook)</center>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"11\"></a>\n<div class=\"subsection_title\">\n    <a href=\"#table_of_contents\" style=\"color: #46A050;\" >4.1. Preparation for LlamaIndex</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nInstall necessary libraries:<br>\n<br>\n    <mark>!pip install -q -U llama-index</mark> installs or updates the <b>llama-index</b> package quietly. <b>llama-index</b> is a library for creating and managing indexes, for search or database purposes.<br>\n<br>\n    <mark>!pip install wikipedia</mark> installs the <b>wikipedia</b> package. The <b>wikipedia</b> package is a Python library that provides a simple interface to access and parse data from Wikipedia, making it easier to work with Wikipedia's content programmatically.<br>\n<br>\n    <mark>!pip install llama-index-readers-wikipedia</mark> installs a Python package named <b>llama-index-readers-wikipedia</b>. This package provides tools or functionality for reading or interacting with an index related to Wikipedia data for processing or querying Wikipedia content.<br>\n<br>    \n    <mark>!pip install llama-index-vector-stores-pinecone</mark> installs the <b>llama-index-vector-stores-pinecone</b> package. This package integrates <b>llama-index</b> with <b>Pinecone</b>, a vector database, allowing indexed data to be stored and queried within <b>Pinecone</b>'s managed vector search service.<br>\n<br>\n    <mark>!pip install -q -U llama-index-embeddings-huggingface</mark> installs or updates the <b>llama-index-embeddings-huggingface</b> package quietly. It sets a bridge between <b>llama-index</b> and <b>Hugging Face</b>'s transformers library, enabling the use of <b>Hugging Face</b>'s pre-trained models for generating embeddings that can be indexed by <b>llama-index</b>.<br>\n<br>\n    <mark>!pip install -q -U pinecone-client</mark> installs or updates the <b>pinecone-client</b> package quietly. <b>pinecone-client</b> is the client library for <b>Pinecone</b>, a vector database service for machine learning applications. It enables interaction with <b>Pinecone</b> services, such as creating and managing vector databases for similarity search.<br>\n<br>\n    <mark>!pip install -q -U sentence-transformers</mark> installs or updates the <b>sentence-transformers</b> package quietly. <b>sentence-transformers</b> is a Python framework for state-of-the-art sentence, text, and image embeddings. It simplifies the process of generating embeddings from sentences or paragraphs, which can be used for tasks like semantic similarity search or clustering.<br>\n<br>\n    <mark>!pip install -q -U keybert</mark> installs or updates the <b>keybert</b> package quietly. <b>KeyBERT</b> is a Python package that leverages state-of-the-art Natural Language Processing (NLP) models to extract keywords and keyphrases from text documents.<br>\n<br>\n    <mark>!pip install -q -U keyphrase-vectorizers</mark> installs or updates the <b>keyphrase-vectorizers</b> package quietly. <b>keyphrase-vectorizers</b> is a set of vectorizers that extract keyphrases with part-of-speech patterns from a collection of text documents and convert them into a document-keyphrase matrix. A document-keyphrase matrix is a mathematical matrix that describes the frequency of keyphrases that occur in a collection of documents. The matrix rows indicate the text documents and columns indicate the unique keyphrases.\n</span>","metadata":{}},{"cell_type":"code","source":"%%capture\n\n\n!pip install -q -U llama-index\n!pip install wikipedia\n!pip install llama-index-readers-wikipedia\n!pip install llama-index-vector-stores-pinecone\n!pip install -q -U llama-index-embeddings-huggingface\n!pip install -q -U pinecone-client\n!pip install -q -U sentence-transformers\n!pip install -q -U keybert\n!pip install -q -U keyphrase-vectorizers","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:22:20.151331Z","iopub.execute_input":"2024-04-09T05:22:20.151639Z","iopub.status.idle":"2024-04-09T05:24:48.197455Z","shell.execute_reply.started":"2024-04-09T05:22:20.151613Z","shell.execute_reply":"2024-04-09T05:24:48.196188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nDefine parameters:\n</span>","metadata":{}},{"cell_type":"code","source":"CHUNK_SIZE = 256\nCHUNK_OVERLAP = 10\nCONTEXT_WINDOW = 4096\nSIMILARITY_TOP_K = 3\nEMBEDDING_MODEL = [\"BAAI/bge-small-en-v1.5\", \"BAAI/bge-base-en-v1.5\"][1]\nTHRESHOLD = 0.5\nINDEX_NAME = \"vectordb-index\"\nPINECONE_ENV = \"gcp-starter\"","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:24:48.199321Z","iopub.execute_input":"2024-04-09T05:24:48.199681Z","iopub.status.idle":"2024-04-09T05:24:48.206045Z","shell.execute_reply.started":"2024-04-09T05:24:48.199646Z","shell.execute_reply":"2024-04-09T05:24:48.204829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nThe LlamaIndex codebase sets the maximum context window size for a Large Language Model (LLM) at 8191 tokens. To prevent surpassing this token limit in a Q&A scenario after implementing LlamaIndex RAG, the total of <mark>CONTEXT_WINDOW</mark> plus the number of tokens in the prompt template must remain below the LLM's max context window. This requirement exists because the space available for context is the overall context window size minus the space taken by the input (the partially filled prompt) and the space reserved for the output (the response). If the combined total of <mark>CONTEXT_WINDOW</mark> and the prompt template's tokens exceeds the LLM's maximum, the available context size becomes negative, leading to a ValueError. Considering our <mark>OUTPUT_MAX_LEN</mark> and the size of our prompt, setting <mark>CONTEXT_WINDOW</mark> to <i>4096</i> should be appropriate.<br>\nSource: <a href=\"https://github.com/run-llama/llama_index/issues/9408\">About setting the context_window and num_output</a><br>\n<br>\nWe'll explore other parameters when we use them. As a note, the parameter values we used throughout the notebook have enabled us to achieve good results from the model.\n</span>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"12\"></a>\n<div class=\"subsection_title\">\n    <a href=\"#table_of_contents\" style=\"color: #46A050;\" >4.2. Gemma as a Custom LLM for LlamaIndex</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\n    LlamaIndex supports using LLMs from <b>HuggingFace</b> directly. However, since our trained model is not available on <b>HuggingFace</b>, we need to implement the LLM class (or CustomLLM for a simpler interface) to use a custom model like ours. LlamaIndex provides us with a small boilerplate example to accomplish this, as shown below:<br>\nSource: <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/models/llms/usage_custom.html\">Customizing LLMs within LlamaIndex Abstractions</a>\n</span>","metadata":{}},{"cell_type":"code","source":"%%capture\n\n\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core.llms import CustomLLM, CompletionResponse, CompletionResponseGen, LLMMetadata\nfrom llama_index.core.llms.callbacks import llm_completion_callback\nfrom typing import Optional, List, Mapping, Any, Tuple\n\n\nclass OurLLM(CustomLLM):\n    context_window: int = CONTEXT_WINDOW\n    num_output: int = OUTPUT_MAX_LEN\n    model_name: str = \"fine-tuned_gemma_en\"\n    model: Any = None\n        \n    def __init__(self, model, context_window, num_output):\n        super(OurLLM, self).__init__()\n        self.model = model\n        self.context_window = context_window\n        self.num_output = num_output     \n\n    @property\n    def metadata(self) -> LLMMetadata:\n        \"\"\"Get LLM metadata.\"\"\"\n        return LLMMetadata(\n            context_window=self.context_window,\n            num_output=self.num_output,\n            model_name=self.model_name,\n        )\n\n    @llm_completion_callback()\n    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n        return CompletionResponse(text=self.model.generate(prompt, max_length=self.num_output))\n\n    @llm_completion_callback()\n    def stream_complete(\n        self, prompt: str, **kwargs: Any\n    ) -> CompletionResponseGen:\n        response = \"\"\n        for token in self.model.generate(prompt, max_length=self.num_output):\n            response += token\n            yield CompletionResponse(text=response, delta=token)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:24:48.207630Z","iopub.execute_input":"2024-04-09T05:24:48.208436Z","iopub.status.idle":"2024-04-09T05:24:55.979279Z","shell.execute_reply.started":"2024-04-09T05:24:48.208397Z","shell.execute_reply":"2024-04-09T05:24:55.977806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nOur custom model for LlamaIndex is:\n</span>","metadata":{}},{"cell_type":"code","source":"gemma_llama = OurLLM(gemma_llm, CONTEXT_WINDOW, OUTPUT_MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:24:55.980675Z","iopub.execute_input":"2024-04-09T05:24:55.981563Z","iopub.status.idle":"2024-04-09T05:24:55.986334Z","shell.execute_reply.started":"2024-04-09T05:24:55.981514Z","shell.execute_reply":"2024-04-09T05:24:55.985431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nLet's try this with one of our queries.\n</span>","metadata":{}},{"cell_type":"code","source":"response = gemma_llama.complete(query_1)\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:24:55.987612Z","iopub.execute_input":"2024-04-09T05:24:55.988263Z","iopub.status.idle":"2024-04-09T05:24:58.320720Z","shell.execute_reply.started":"2024-04-09T05:24:55.988200Z","shell.execute_reply":"2024-04-09T05:24:58.319697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\n    Just in case we need to delete some variables, models, or dataframes, we import the <mark>gc</mark> library.<br>\n<br>\n    The <mark>gc</mark> (garbage collection) module in Python provides an interface to the garbage collector, which is responsible for automatically freeing up memory by destroying objects that are no longer in use by the program. Python's garbage collector is mainly used to clean up cyclic references between objects and other internal objects that may not be accessible from Python code but still consume memory.<br>\n<br>\n    <mark>gc.collect()</mark> command manually forces a garbage collection.\n</span>","metadata":{}},{"cell_type":"code","source":"import gc\n\n\n# # Delete the previous model to save memory\n# del gemma_llm\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:24:58.321953Z","iopub.execute_input":"2024-04-09T05:24:58.322258Z","iopub.status.idle":"2024-04-09T05:24:58.731616Z","shell.execute_reply.started":"2024-04-09T05:24:58.322233Z","shell.execute_reply":"2024-04-09T05:24:58.730544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"13\"></a>\n<div class=\"subsection_title\">\n    <a href=\"#table_of_contents\" style=\"color: #46A050;\" >4.3. Embeddings and LlamaIndex Settings</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nThe dimension of an embedding model significantly affects its ability to capture the nuances of the data it represents. A higher dimension can encode more detailed features and relationships, potentially improving the model's performance on tasks like similarity search or classification. However, larger dimensions also mean increased computational cost and memory usage. Balancing dimensionality is crucial for optimizing performance and efficiency, ensuring the model captures enough complexity without becoming overly resource-intensive.<br>\n<br>\nIn our project, we'll utilize the <b>Hugging Face Embedding</b> capabilities to generate embeddings for document segments. It's easy and free to use. As of March 2024, the <mark>BAAI/bge-base-en-v1.5</mark> model is highly regarded in Retrieval tasks and has a dimensionality of <i>764</i>. This offers a good balance between computational efficiency and the model's ability to capture data.<br>\nSource: <a href=\"https://huggingface.co/spaces/mteb/leaderboard\">mteb/leaderboard</a><br>\n<br>\nRetrieval Augmented Generation (RAG) enhances a Large Language Model (LLM) by incorporating additional context or information from a separate source. Retrieval involves adding more context or information to language models to enrich their output. LLamaIndex tackles the issue of scaling language models for extensive document collections. To address this challenge, LLamaIndex implements two main approaches. First, it breaks down documents into smaller units (chunks) like sentences or paragraphs, known as Nodes, which language models can easily handle. Second, it organizes these Nodes by creating vector embeddings, allowing for rapid and meaningful searches.<br>\nSource: <a href=\"https://medium.com/@bavalpreetsinghh/llamaindex-chunking-strategies-for-large-language-models-part-1-ded1218cfd30\">LlamaIndex: Chunking Strategies for Large Language Models. Part — 1</a><br>\n<br>\nSelecting the appropriate chunk size is crucial for optimizing both the effectiveness and precision of a RAG system in various aspects:\n</span>","metadata":{}},{"cell_type":"markdown","source":"<ul>\n    <li><b>Relevance and Detail</b>: Choosing a smaller <mark>chunk_size</mark>, such as 128, creates more detailed chunks. This detail can be a double-edged sword: critical information may be missed if it doesn't appear in the top chunks, particularly when the <mark>similarity_top_k</mark> parameter is set to a low value like 2. On the other hand, a larger <mark>chunk_size</mark>, such as 512, has a better chance of including all necessary information in the top chunks, ensuring easier access to answers. To address these issues, we use Faithfulness and Relevancy metrics, which evaluate the accuracy and appropriateness of responses in relation to the original query and the contexts retrieved.</li>\n    <li><b>Time to Generate Responses</b>: Increasing the <mark>chunk_size</mark> also increases the amount of information fed into the LLM for generating a response. While this may provide a richer context, it could potentially reduce the system's speed. It's vital to maintain the system's quick response capability without compromising the depth of information provided.</li>\n</ul>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\n    Ultimately, finding the ideal <mark>chunk_size</mark> involves finding a middle ground: ensuring all vital information is captured without affecting the system's speed.<br>\nSource: <a href=\"https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5\">Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex</a><br> \n<br>\nA smaller chunk size might reduce memory usage and improve responsiveness for incremental updates, while a larger chunk size could enhance throughput for bulk operations but require more memory. \nThe default chunk size is 1024, while the default chunk overlap is 20. Changing either of these parameters will change the embeddings that are calculated. The smaller the <mark>CHUNK_SIZE</mark>, the greater the number of records in the vector database. Conversely, the larger the <mark>CHUNK_SIZE</mark>, the fewer records there will be in the vector database. A smaller chunk size means the embeddings are more precise, while a larger chunk size means that the embeddings may be more general, but can miss fine-grained details. Furthermore, when changing the chunk size for a vector index, you may also want to increase the <mark>similarity_top_k</mark> parameter to better represent the amount of data to retrieve for each query. For example, if we halve the default chunk size, we may double the <mark>similarity_top_k</mark> from the default of 2 to 4. <mark>similarity_top_k=4</mark> means the index will fetch the top 4 closest matching terms/definitions to the query.<br>\nSource: <a href=\"https://docs.llamaindex.ai/en/stable/optimizing/basic_strategies/basic_strategies/#chunk-sizes\">Chunk Sizes</a><br>\n<br>\nA smaller value for <mark>similarity_top_k</mark> might make queries faster by considering fewer documents but could potentially miss relevant documents. Conversely, a larger value increases the chances of finding highly relevant documents but may slow down query processing. Given the <mark>CHUNK_SIZE</mark> we set, we have seen that the model produces concise, relevant, and fast responses when <mark>similarity_top_k</mark> is <i>1</i>.<br>\n<br>\nWe set <mark>CHUNK_SIZE</mark> as <i>256</i>, considering it can help manage memory usage and improve performance by optimizing the balance between the number of operations and resource utilization.<br>\n<br>\n<mark>CHUNK_OVERLAP</mark> refers to the portion of content that neighboring chunks share. This concept can be used to ensure that no information is lost or to enhance the accuracy of operations like search or analysis that might span across the boundaries of chunks. Although the default overlap is 20, we have decreased it since we have a smaller <mark>CHUNK_SIZE</mark>.<br>\n<br>\n    When dividing the text into chunks, we utilize the <mark>SentenceSplitter</mark> parser. This parser segments the text with a preference for complete sentences, aiming to keep sentences and paragraphs intact. As a result, compared to the original TokenTextSplitter, it is less likely to produce chunks with incomplete sentences or sentence fragments at the end of a node chunk.<br>\nSource: <a href=\"https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/sentence_splitter\">Sentence splitter</a><br>  \n</span>","metadata":{}},{"cell_type":"code","source":"%%capture\n\n\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core import Settings\n\n\nSettings.embed_model = HuggingFaceEmbedding(model_name=EMBEDDING_MODEL)\n\nsentence_splitter = SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n\nSettings.text_splitter = sentence_splitter","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:24:58.732980Z","iopub.execute_input":"2024-04-09T05:24:58.733306Z","iopub.status.idle":"2024-04-09T05:25:05.104679Z","shell.execute_reply.started":"2024-04-09T05:24:58.733278Z","shell.execute_reply":"2024-04-09T05:25:05.103780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"text-with-green-strip\">\n<p style=\"font-size: 14px; margin-left: 70px;\">\n    This code snippet configures a text processing pipeline using components from the <span class=\"marks\">llama_index</span> library, specifically for handling embeddings and text splitting, by performing the following operations:<br>\n    <b>Embedding Model Setup</b>: It sets up a HuggingFace embedding model (<span class=\"marks\">HuggingFaceEmbedding</span>) as the embedding model for the application, specifying the model name via <span class=\"marks\">EMBEDDING_MODEL</span>. This model is used to generate embeddings for text data.<br>\n    <b>Sentence Splitting Configuration</b>: A <span class=\"marks\">SentenceSplitter</span> is configured with specified <span class=\"marks\">chunk_size</span> and <span class=\"marks\">chunk_overlap</span>, determining how text is divided into chunks and the overlap between these chunks, respectively, for efficient text processing.<br>\n    <b>Global Settings Update</b>: The script updates the global <span class=\"marks\">Settings</span> to use the specified embedding model and sentence splitter, configuring the environment for subsequent text processing operations within the <span class=\"marks\">llama_index</span> framework.<br>\n</p>\n<p style=\"font-size: 14px; margin-left: 70px;\">\n    Overall, this snippet prepares the text processing environment by selecting an embedding model for converting text into numerical representations and specifying how text should be split, tailoring the setup for further text analysis or information retrieval tasks.\n</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\n    The <mark>Settings</mark> is a bundle of commonly used resources used during the indexing and querying stage in a LlamaIndex pipeline/application. It can be used to set the global configuration. Local configurations (transformations, LLMs, embedding models) can be passed directly into the interfaces that make use of them. The <mark>Settings</mark> is a simple singleton object that lives throughout the application. Whenever a particular component is not provided, the <mark>Settings</mark> object is used to provide it as a global default.<br>\nSource: <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/settings/#configuring-settings\">Configuring Settings</a><br>\n<br>\nWe require the dimension size of our embedding model, which will be utilized in creating the index. Within this index, each document chunk will be represented by a vector corresponding to the dimension size of the employed embedding model.<br>\n<br>\n    We import the <mark>SentenceTransformer</mark> class from the <mark>sentence_transformers</mark> library, which is a tool designed for generating sentence embeddings using transformer-based models. These embeddings can be used for a variety of natural language processing tasks, including semantic similarity, clustering, and information retrieval. Next, we import <mark>Numpy</mark>, which is a fundamental package for scientific computing in Python, offering support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.\n</span>","metadata":{}},{"cell_type":"code","source":"%%capture\n\n\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\n\ndef generate_embeddings(texts: List[str], embedding_model: str) -> np.ndarray:\n    \n    \"\"\"\n    Generates embeddings for a list of texts using a specified embedding model.\n\n    This function initializes a SentenceTransformer model with the given embedding model name,\n    and then computes embeddings for each text in the input list. The embeddings are generated\n    in a batched manner for efficiency, resulting in a numpy ndarray where each row corresponds\n    to the embedding of a text in the input list.\n\n    Parameters:\n    - texts (List[str]): A list of texts for which embeddings are to be generated.\n    - embedding_model (str): The name of the SentenceTransformer model to be used for generating\n      embeddings. This should correspond to one of the pre-trained models available in the\n      SentenceTransformers library or a model identifier that can be loaded by Hugging Face's model hub.\n\n    Returns:\n    - np.ndarray: A 2D numpy array containing the generated embeddings. Each row in the array\n      corresponds to the embedding of the respective text in the input list.\n\n    Example:\n    >>> texts = [\"This is an example sentence\", \"Here is another one\"]\n    >>> embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n    >>> embeddings = generate_embeddings(texts, embedding_model)\n    >>> print(embeddings.shape)\n    (2, 384)\n\n    Note:\n    - The dimensionality of the embeddings (number of columns in the returned ndarray) is determined\n      by the specified embedding model.\n    \"\"\"\n \n    # Compute embeddings\n    model = SentenceTransformer(embedding_model)\n    embeddings = model.encode(texts)\n    \n    return embeddings  \n    \n    \ndef get_embed_dim(embedding_model: str) -> int:\n    \n    \"\"\"\n    Determines the dimensionality of embeddings generated by a specific SentenceTransformer model.\n\n    This function uses a sample sentence to generate an embedding using the specified SentenceTransformer\n    model, and then determines the embedding's dimensionality. This dimensionality is indicative of the\n    size of the vector space in which the texts are embedded, which varies depending on the model used.\n\n    Parameters:\n    - embedding_model (str): The identifier of the SentenceTransformer model for which the embedding\n      dimensionality is to be determined.\n\n    Returns:\n    - int: The dimensionality of the embeddings produced by the specified model.\n\n    Example:\n    >>> embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n    >>> dim = get_embed_dim(embedding_model)\n    Embedding dimension of the model is:  384\n\n    Note:\n    - The function assumes the presence of a `generate_embeddings` function capable of producing embeddings\n      from text inputs using SentenceTransformer models.\n    - The choice of sample sentence does not affect the determined dimensionality, as all embeddings\n      generated by a specific model will have the same dimensionality.\n    \"\"\"\n    \n    # Sample sentence\n    sentence = ['Show me the money!']\n\n    # Compute embeddings\n    embeddings = generate_embeddings(sentence, embedding_model)\n    \n    # Find the embedding dimension\n    dim = len(embeddings[0])\n\n    print(\"\\n\", \"Embedding dimension of the model is: \", dim, \"\\n\")\n\n    return dim\n\n\n# Get embedding dimension\ndim = get_embed_dim(EMBEDDING_MODEL)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:25:05.105829Z","iopub.execute_input":"2024-04-09T05:25:05.106156Z","iopub.status.idle":"2024-04-09T05:25:10.242518Z","shell.execute_reply.started":"2024-04-09T05:25:05.106129Z","shell.execute_reply":"2024-04-09T05:25:10.241622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(dim)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:25:10.243735Z","iopub.execute_input":"2024-04-09T05:25:10.244064Z","iopub.status.idle":"2024-04-09T05:25:10.249269Z","shell.execute_reply.started":"2024-04-09T05:25:10.244037Z","shell.execute_reply":"2024-04-09T05:25:10.248148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"14\"></a>\n<div class=\"subsection_title\">\n    <a href=\"#table_of_contents\" style=\"color: #46A050;\" >4.4. Workflow of Response Generation with RAG and Gemma</a>\n</div>\n\n","metadata":{}},{"cell_type":"markdown","source":"<center><img src=\"https://i.ibb.co/S7y0dxQ/rag-cycle.png\" border=\"0\" width=1000></center>\n<center>Workflow of Response Generation with RAG and Gemma (Image by Author)</center>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nThe flowchart describes the process for handling and responding to a query:\n</span>","metadata":{}},{"cell_type":"markdown","source":"<ul id=\"customUnorderedList\">\n    <li><b>1. Receive Query</b>: The process starts with receiving a user's query. This query, for the moment, is one of the seven queries specified at the beginning of the notebook.</li>\n    <li><b>2. Extract and Augment Keywords</b>: Extract keywords from the query and expand them for better context understanding.</li>\n    <li><b>3. Execute Semantic Similarity for Relevancy</b>: Assess the relevancy of keywords by evaluating their semantic similarity to the data science context, including only data science-related keywords.</li>\n    <li><b>4. Search for Keywords in Wikipedia</b>: Look up the relevant keywords in Wikipedia to find corresponding articles.</li>\n    <li><b>5. Are Keywords Loadable?</b>: Check if the relevant articles and pages can be loaded, from the search results performed in the previous step.</li>\n    <li><b>6a. Discard the Keyword</b>: If a keyword isn't loadable, or Wikipedia doesn't have an article or page dedicated to the keyword, it's discarded.</li>\n    <li><b>6b. Add the Keyword to the Successful List</b>: If a keyword is loadable, or Wikipedia does have an article or page dedicated to the keyword, it's added to a list of successfully retrieved keywords.</li>\n    <li><b>7. Load Relevant Wikipedia Pages</b>: Load content from Wikipedia pages that are relevant to the successful keywords.</li>\n    <li><b>8. Create Index and Vector Store</b>: Create an index and vector store in Pinecone to store documents in chunks vectorized for efficient search and retrieval.</li>\n    <li><b>9. Upsert Docs into the Database</b>: Insert or update the indexed documents into the database.</li>\n    <li><b>10. Create the Query Engine</b>: Set up the query engine, which is an information retrieval system to process queries against the indexed data.</li>\n    <li><b>11. Generate Response</b>: Produce a response to the original query using the indexed content and the query engine.</li>\n    <li><b>12a. LlamaIndex</b>: Use LlamaIndex to retrieve the relevant content from the vector database and feed it into Gemma.</li>\n    <li><b>12b. Gemma</b>: Use Gemma to search for the answer in the content retrieved from the vector database.</li>\n    <li><b>13. Display Response</b>: Show the generated response to the user.</li>\n    <li><b>14. Evaluate the Response</b>: Review and assess the quality of the response.</li>\n    <li><b>15. Delete the Index and Query Engine</b>: Clean up by deleting the index and dismantling the query engine, free resources and prepare the environment for subsequent queries.</li>\n</ul>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nThe process ends after the response is displayed and evaluated. This flow reflects a our cycle in a search engine or question-answering system that includes steps from initial query processing to response generation and evaluation.<br>\n<br>    \nIn the following, we will delve into details of these steps and the accompanying code scripts.\n</span>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"15\"></a>\n<div class=\"subsection_title\">\n    <a href=\"#table_of_contents\" style=\"color: #46A050;\" >4.5. Extract Keywords</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">   \n    We have chosen <b>Wikipedia</b> as our information source for the <b>Retrieval-Augmented Generation (RAG)</b> technique. <b>Wikipedia</b> offers a vast repository of knowledge and is regularly updated. However, instead of importing the entire corpus into the vector store, we will selectively find, load, and store only the pages relevant to the current query. This will be accomplished as follows:<br>\n<br>\n<b>Extract keywords (keyphrases)</b>: To extract keywords, we initially employ the <b>KeyBERT</b> and <b>KeyphraseCountVectorizer</b> libraries through the <mark>extract_keyphrases</mark> function. This combination of <b>KeyphraseVectorizers</b> with <b>KeyBERT</b> forms the <b>PatternRank</b> method, which ranks keyphrases using pretrained language models (<b>KeyBERT</b>) and selects candidate keyphrases based on part-of-speech patterns (<b>KeyphraseVectorizers</b>). <b>PatternRank</b> thus capitalizes on the strengths of both tools. <b>KeyphraseCountVectorizer</b> allows the model to determine the n-gram setting. The <mark>extract_keyphrases</mark> function then returns significant keywords, with statistics omitted.<br>\nSource: <a href=\"https://towardsdatascience.com/unsupervised-keyphrase-extraction-with-patternrank-28ec3ca737f0\">Unsupervised Keyphrase Extraction with PatternRank</a><br>\n<br>\n<b>Search for semantic similarity</b>: We then assess the relevance of the keyphrases to the data science field by calculating embeddings for these keyphrases and for identifier terms like “data science,” “machine learning,” “artificial intelligence,” and “statistics” using <mark>SentenceTransformer</mark> from <b>Hugging Face Embedding</b>. The cosine similarity between each keyword and the identifier terms is calculated, accepting keywords with a similarity above <i>0.5</i> (<mark>THRESHOLD</mark>). The <mark>generate_embeddings</mark>, <mark>calculate_cos_sim</mark>, and <mark>filter_by_cos_sim</mark> functions handle these embeddings, calculations and filtering.<br>\n<br>\n<b>Augment one-word keywords</b>: Finally, to reduce ambiguity, we prepend “data science” to one-word keyphrases. Phrases other than \"data science\" can also be prepended, thereby broadening the range of topics to search and increasing the likelihood of finding relevant articles on <b>Wikipedia</b>. However, we will not explore this approach further in this notebook. This step aims to refine the search within <b>Wikipedia</b>, though it may occasionally retrieve irrelevant pages. For example, 'dropout' can mean 'a college dropout' or refer to 'a regularization technique.' The <mark>augment_keywords</mark> function carries out this augmentation. In addition, <b>KeyBERT</b> is a powerful library to ignore keywords that are relatively generic, such as \"explain\" or \"describe\".<br>\n<br>\n    The <mark>process_keywords</mark> function will coordinate the other functions and return the finalized keywords for downstream tasks.<br>\n<br>\nThe functions mentioned above are detailed and demonstrated in the following section, covering steps 2 and 3 of the Flowchart.\n</span>","metadata":{}},{"cell_type":"code","source":"%%capture\n\n\nfrom keyphrase_vectorizers import KeyphraseCountVectorizer\nfrom keybert import KeyBERT\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\n# Broad category names that encompass many data science terms and concepts\nidentifiers = [\"data science\", \"machine learning\", \"artificial intelligence\", \"deep learning\", \"statistics\"]\n               \n# Compute embeddings for identifiers\nidentifier_embeddings = generate_embeddings(identifiers, EMBEDDING_MODEL)\n\n# Initialize default vectorizer.\nvectorizer = KeyphraseCountVectorizer()\n\n# Initialize KeyBERT\nkw_model = KeyBERT()\n\n\ndef extract_keyphrases(text: List[str]) -> List[str]:\n    \n    \"\"\"\n    Extracts key phrases from a given list of text documents.\n\n    This function preprocesses the input text by removing hyphens, then uses a keyword\n    extraction model to extract n-gram key phrases. The key phrases are extracted without\n    their associated statistics (like scores or frequencies), providing a clean list of\n    key phrases found within the text.\n\n    Parameters:\n    - text (List[str]): A list containing strings of text from which to extract key phrases.\n\n    Returns:\n    - List[str]: A list of extracted key phrases as strings.\n\n    Note:\n    - This function assumes the presence of a globally defined keyword model `kw_model`\n      that has a method `extract_keywords`. This model should be initialized and configured\n      outside the function.\n    - It also uses `KeyphraseCountVectorizer` for vectorizing the text. Make sure this\n      vectorizer is available and properly imported in your environment.\n\n    Example:\n    >>> texts = [\"Natural language processing with Python.\", \"Text data processing and analysis.\"]\n    >>> key_phrases = extract_keyphrases(texts)\n    >>> print(key_phrases)\n    \"\"\"\n    \n    # Replace hyphen with empty space\n    text = text.replace(\"-\", \"\")\n\n    # Extract n-gram key phrases from the query\n    key_phrases = kw_model.extract_keywords(docs=text, vectorizer=KeyphraseCountVectorizer())\n\n    # Extract key phrases only, and remove statistics\n    key_phrases = [key_phrase[0] for key_phrase in key_phrases]\n    \n    return key_phrases\n    \n               \ndef calculate_cos_sim(extracted_keywords: List[str]) -> np.ndarray:\n    \n    \"\"\"\n    Calculates the cosine similarity between a list of extracted keywords and a predefined list\n    of identifiers, both represented as embeddings.\n\n    This function first generates embeddings for the given list of extracted keywords using a\n    specified embedding model. Then, it calculates the cosine similarity between these keyword\n    embeddings and another set of embeddings (for identifiers) stored in a variable\n    named `identifier_embeddings`. The result is a similarity matrix where each element\n    represents the cosine similarity between a keyword embedding and an identifier embedding.\n\n    Parameters:\n    - extracted_keywords (List[str]): A list of strings representing extracted keywords for which\n      embeddings will be generated and compared.\n\n    Returns:\n    - np.ndarray: A 2-dimensional NumPy array representing the cosine similarity matrix between\n      keyword embeddings and identifier embeddings. The dimensions of the matrix are determined\n      by the number of keywords and identifiers, with rows corresponding to keywords and columns\n      to identifiers.\n\n    Note:\n    - The function assumes the existence of a globally accessible `EMBEDDING_MODEL` that is used\n      to generate embeddings for the keywords.\n    - `identifier_embeddings` is presumed to be a pre-generated set of embeddings for a list of\n      identifiers and should be available in the scope where this function is called.\n    - This function relies on the `cosine_similarity` function from a library like `sklearn` to\n      compute the similarity matrix. Make sure this function is imported before calling\n      `calculate_cos_sim`.\n\n    Example:\n    >>> extracted_keywords = [\"machine learning\", \"natural language processing\"]\n    >>> keyword_embeddings = generate_embeddings(extracted_keywords, embedding_model)\n    >>> similarity_matrix = cosine_similarity(keyword_embeddings, identifier_embeddings)\n    >>> print(similarity_matrix.shape)\n    \"\"\"\n               \n    # Generate embeddings for keywords\n    keyword_embeddings = generate_embeddings(extracted_keywords, EMBEDDING_MODEL)\n               \n    # Calculate cosine similarity between each term of both lists\n    similarity_matrix = cosine_similarity(keyword_embeddings, identifier_embeddings)\n               \n    return similarity_matrix\n               \n               \ndef filter_by_cos_sim(extracted_keywords: List[str], threshold: float = 0.5) -> List[str]:\n    \n    \"\"\"\n    Filters a list of extracted keywords by cosine similarity, selecting those keywords that\n    have a similarity score exceeding a specified threshold with any identifier embeddings.\n\n    This function leverages the `calculate_cos_sim` function to generate a cosine similarity\n    matrix between the embeddings of the provided keywords and a predefined set of identifier\n    embeddings. It then iterates over this matrix to select and return keywords whose similarity\n    score with any of the identifier embeddings exceeds the given threshold.\n\n    Parameters:\n    - extracted_keywords (List[str]): A list of strings representing extracted keywords to be filtered\n      based on their cosine similarity with identifier embeddings.\n    - threshold (float, optional): The minimum cosine similarity score required for a keyword to be\n      selected. Defaults to 0.5.\n\n    Returns:\n    - List[str]: A list of keywords from the input `extracted_keywords` list that meet the specified\n      cosine similarity threshold with any of the identifier embeddings.\n\n    Note:\n    - The function assumes that `calculate_cos_sim` is correctly implemented and available, which\n      computes the cosine similarity matrix between the given keywords and a predefined set of\n      identifier embeddings.\n    - The function relies on `np.any` from the NumPy library to check if any of the similarity scores\n      in a row exceed the threshold, implying that the corresponding keyword is sufficiently similar\n      to at least one identifier.\n    - `identifier_embeddings` used in the similarity calculation by `calculate_cos_sim` should be\n      pre-defined and accessible within the scope of that function.\n\n    Example:\n    >>> extracted_keywords = [\"deep learning\", \"data science\", \"AI ethics\"]\n    >>> selected_keywords = filter_by_cos_sim(extracted_keywords, threshold=0.5)\n    >>> print(selected_keywords)\n    \"\"\"\n    \n    # Generate embeddings for both lists\n    similarity_matrix = calculate_cos_sim(extracted_keywords)\n               \n    # Initialize a list to hold strings from keywords list that meet the threshold condition\n    selected_elements = []\n\n    # Iterate over similarity matrix\n    for i, similarities in enumerate(similarity_matrix):\n        # Check if any similarity value for the current term in keywords list exceeds the threshold\n        if np.any(similarities > threshold):\n            # If so, add the term from keywords list to the selected list\n            selected_elements.append(extracted_keywords[i])\n               \n    return selected_elements\n               \n    \ndef augment_keywords(raw_keywords: List[str]) -> List[str]:\n\n    \"\"\"\n    Augments a list of raw keywords to make them more specific to the domain of data science.\n    For any keyword in the input list that consists of a single word, this function prepends\n    the term \"data science\" to it, thus transforming it into a data science-related term.\n    Keywords that are already more than a single word are left unchanged.\n\n    Parameters:\n    - raw_keywords (List[str]): A list of strings, each representing a raw keyword or keyphrase.\n\n    Returns:\n    - List[str]: A list of strings where single-word keywords have been augmented with the\n      \"data science\" prefix, and multi-word keywords/keyphrases are included as originally provided.\n\n    Example:\n    >>> raw_keywords = [\"machine learning\", \"python\", \"deep\"]\n    >>> augmented_keywords = augment_keywords(raw_keywords)\n    >>> print(augmented_keywords)\n    ['machine learning', 'data science python', 'data science deep']\n    \"\"\"\n    \n    # Initialize an empty list to store augmented keywords\n    augmented_keywords = []\n    \n    # Loop over extracted keywords\n    for raw_keyword in raw_keywords:\n    \n        # If a keyword's length by word is less than two, add the 'data science' term to the\n        # beginning of the keyword, making it a data science-related term\n        if len(raw_keyword.split()) < 2:\n            augmented_keyword = \"data science \" + raw_keyword\n\n            # Add the updated (data science-relevant transformed) keyword to the list\n            augmented_keywords.append(augmented_keyword)\n        else:\n            # If the length of the keyword is more than one, add it to the list as is\n            augmented_keywords.append(raw_keyword)\n    \n    return augmented_keywords\n\n\ndef process_keywords(text: List[str], threshold: float = 0.5) -> List[str]:\n    \n    \"\"\"\n    Processes a list of text documents to extract, filter, and augment keywords or key phrases\n    based on their relevance to a specific domain, in this case, data science. The process\n    involves three main steps: extraction of key phrases, filtering based on cosine similarity\n    with a predefined set of domain-relevant terms, and augmenting single-word key phrases\n    to make them more domain-specific.\n\n    Parameters:\n    - text (List[str]): A list of strings where each string is a text document from which\n      key phrases are to be extracted.\n    - threshold (float, optional): The cosine similarity threshold used to filter out key\n      phrases that are not sufficiently relevant to the predefined domain. Key phrases with\n      a cosine similarity above this threshold to any domain-relevant term are retained.\n      Defaults to 0.5.\n\n    Returns:\n    - List[str]: A list of strings representing the processed key phrases. Single-word key\n      phrases that pass the relevance filter are augmented with a domain-specific identifier\n      (\"data science\"), while multi-word key phrases are included as originally extracted and\n      filtered.\n\n    Note:\n    - The function relies on `extract_keyphrases` to extract key phrases from the input text.\n    - `filter_by_cos_sim` is used to filter the extracted key phrases based on their cosine\n      similarity to a predefined set of domain-relevant terms.\n    - `augment_keywords` is applied to the filtered list to augment single-word key phrases\n      with a domain-specific identifier.\n\n    Example:\n    >>> text = [\"An introduction to machine learning\", \"Python programming basics\"]\n    >>> processed_keywords = process_keywords(text, threshold=0.5)\n    >>> print(processed_keywords)\n    \"\"\"\n\n    # Extract key phrases from the query\n    key_phrases = extract_keyphrases(text)\n    \n    # Filter out key phrases that are not relevant to data science\n    filtered_key_phrases = filter_by_cos_sim(key_phrases, threshold=threshold)\n    \n    # Prepend an identifier to relevant one-word key phrases\n    augmented_keywords = augment_keywords(filtered_key_phrases)\n    \n    return augmented_keywords","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:25:10.250565Z","iopub.execute_input":"2024-04-09T05:25:10.250876Z","iopub.status.idle":"2024-04-09T05:25:18.896585Z","shell.execute_reply.started":"2024-04-09T05:25:10.250826Z","shell.execute_reply":"2024-04-09T05:25:18.895525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"16\"></a>\n<div class=\"subsection_title\">\n    <a href=\"#table_of_contents\" style=\"color: #46A050;\" >4.6. Search for Keywords in Wikipedia</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nWe have extracted keywords related to data science from the query. These keywords or keyphrases define the query's scope and form the basis for locating answers within documentation sourced from <b>Wikipedia</b>. At this stage, it's not yet clear whether <b>Wikipedia</b> contains an article for each of these phrases. To proceed with step 4 of the Flowchart, we must verify their existence.<br>\n<br>\nIf <b>Wikipedia</b> articles corresponding to the keywords exist, we can load those pages. To determine whether an article for a keyphrase exists, we can employ the same <mark>load_data</mark> method used to load pages into LlamaIndex. If this method successfully retrieves a page, the keyword will be considered valid and retained. Otherwise, it will be discarded. The <mark>try_loading_keyword</mark> function is designed to carry out this verification.<br>\n<br>\nBy doing this, we restrict our knowledge base to pages related to these keywords instead of leveraging the entire corpus. This approach ensures that only pages specific to the query are retrieved, and only at the time a query is initiated.<br>\n<br>\n    Execution of the <mark>try_loading_keyword</mark> function completes steps 4, 5, 6a, and 6b of the Flowchart.\n</span>","metadata":{}},{"cell_type":"code","source":"from llama_index.readers.wikipedia import WikipediaReader\n\n\nloader = WikipediaReader()\n\n\n# Function to check if loading data for a keyword raises an error\ndef try_loading_keyword(keyword: str) -> bool:\n    \n    \"\"\"\n    Attempts to load data for a given keyword and returns a boolean indicating\n    the success or failure of the operation.\n\n    This function tries to call a `load_data` method on a global `loader`\n    object, passing the keyword as an argument. If the data loading process completes\n    without raising any exceptions, the function returns True, indicating success.\n    If an exception is encountered during the data loading process, it catches the\n    exception, logs an error message with the keyword and the exception details, and\n    returns False, indicating failure.\n\n    Parameters:\n    - keyword (str): The keyword for which data loading is attempted.\n\n    Returns:\n    - bool: True if data loading succeeds without errors; False otherwise.\n\n    Note:\n    - The function assumes the existence of a `loader` object with a `load_data` method\n      that accepts a list of pages (keywords) to load data for. This object and method\n      should be defined and accessible in the scope where this function is called.\n    - Any exception raised by the `load_data` method is caught, logged, and does not\n      propagate further, allowing the program to continue execution.\n\n    Example:\n    >>> success = try_loading_keyword(\"machine learning\")\n    >>> if success:\n    ...     print(\"Data loaded successfully.\")\n    ... else:\n    ...     print(\"Failed to load data.\")\n    \"\"\"\n    \n    try:\n        # Attempt to load data for the keyword\n        # Assuming `loader.load_data()` can be called like this for demonstration\n        loader.load_data(pages=[keyword])\n        return True\n    except Exception as e:\n        # Handle or log the exception if needed\n        print(f\"Error loading data for {keyword}: {e}\")\n        return False","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:25:18.897923Z","iopub.execute_input":"2024-04-09T05:25:18.898587Z","iopub.status.idle":"2024-04-09T05:25:19.155306Z","shell.execute_reply.started":"2024-04-09T05:25:18.898558Z","shell.execute_reply":"2024-04-09T05:25:19.154414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"17\"></a>\n<div class=\"subsection_title\">\n    <a href=\"#table_of_contents\" style=\"color: #46A050;\" >4.7. Create and Load Documents</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nNow, we have identified successful keywords, which include only those that could be loaded without errors and correspond to a specific page on <b>Wikipedia</b>. Next, we can invoke <mark>loader.load_data()</mark> using these successfully loaded keywords. The <mark>create_doc</mark> function is designed to perform this task, thereby completing step 7 of the Flowchart.\n</span>","metadata":{}},{"cell_type":"code","source":"def create_doc(successful_keywords: List[str]) -> List:\n    \n    \"\"\"\n    Loads and returns documents for a list of successful keywords.\n\n    This function uses a globally available `loader` object to load data for each\n    keyword in the provided list. The `loader` object's `load_data` method is assumed\n    to accept a list of keywords (referred to as pages) and returns a list of documents\n    associated with those keywords. The documents returned by the `load_data` method are\n    then directly returned by this function.\n\n    Parameters:\n    - successful_keywords (List[str]): A list of strings, where each string is a keyword\n      for which data loading is to be attempted.\n\n    Returns:\n    - List: A list of documents loaded for the given keywords. The specific type and format\n      of the documents depend on the implementation of the `loader` object's `load_data` method.\n\n    Note:\n    - The function assumes the existence of a `loader` object with a method `load_data` capable\n      of loading data based on a list of keywords. This `loader` object and its method should be\n      properly initialized and accessible in the scope where this function is called.\n    - It is the caller's responsibility to ensure that the keywords in `successful_keywords` are\n      valid and that `loader.load_data` can successfully retrieve data for them.\n\n    Example:\n    >>> successful_keywords = [\"data science\", \"machine learning\"]\n    >>> documents = create_doc(successful_keywords)\n    >>> print(documents)\n    \"\"\"\n        \n    documents = loader.load_data(pages=successful_keywords)\n\n    return documents","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:25:19.156519Z","iopub.execute_input":"2024-04-09T05:25:19.157314Z","iopub.status.idle":"2024-04-09T05:25:20.276953Z","shell.execute_reply.started":"2024-04-09T05:25:19.157284Z","shell.execute_reply":"2024-04-09T05:25:20.276062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"18\"></a>\n<div class=\"subsection_title\">\n    <a href=\"#table_of_contents\" style=\"color: #46A050;\" >4.8. Create the Index and Vector Store</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nThis ingestion pipeline typically consists of three main stages:<br>\n</span>\n<ol>\n    <li>Load the data</li>\n    <li>Transform the data</li>\n    <li>Index and store the data</li>\n</ol>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nWe have already completed the initial stage. After performing transformations, we aim to store the data, which will be in vector form. Vector storage is typically created in memory. However, when I attempted to initiate a vector store in memory, I faced capacity issues. A viable solution is to set up an external vector database. <b>Pinecone</b> is one option we can consider, offering a free tier accessible via an API key. We’ll connect to <b>Pinecone</b> by initially retrieving our token from Kaggle Secrets.<br>\nSource: <a href=\"https://docs.llamaindex.ai/en/stable/understanding/loading/loading/#loading-data-ingestion\">Loading Data (Ingestion)</a><br>\n</span>","metadata":{}},{"cell_type":"code","source":"%%capture\n\n\nfrom kaggle_secrets import UserSecretsClient\n\n\n# Label\npinecone_label = \"PINECONE_API\"\n\n# Retrieve the token\npinecone_api = UserSecretsClient().get_secret(pinecone_label)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:25:20.278214Z","iopub.execute_input":"2024-04-09T05:25:20.278540Z","iopub.status.idle":"2024-04-09T05:25:20.519450Z","shell.execute_reply.started":"2024-04-09T05:25:20.278507Z","shell.execute_reply":"2024-04-09T05:25:20.518570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"text-with-green-strip\">\n<p style=\"font-size: 14px; margin-left: 70px;\">\n    This code is used to securely fetch the <b>Pinecone</b> API token that has been stored as a secret in Kaggle's environment.\n</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nNext, using the generated Pinecone API key, we will initialize a client instance to connect and create the vector index.\n</span>","metadata":{}},{"cell_type":"code","source":"%%capture\n\n\nfrom pinecone import Pinecone, PodSpec\n\n\npc = Pinecone(api_key=pinecone_api)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:25:20.520694Z","iopub.execute_input":"2024-04-09T05:25:20.521036Z","iopub.status.idle":"2024-04-09T05:25:20.555675Z","shell.execute_reply.started":"2024-04-09T05:25:20.521012Z","shell.execute_reply":"2024-04-09T05:25:20.554792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"text-with-green-strip\">\n<p style=\"font-size: 14px; margin-left: 70px;\">\n    This code snippet sets up the necessary configuration to authenticate and start interacting with <b>Pinecone</b>'s services, leveraging the API key stored securely in Kaggle's environment.\n</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nIndexing is a complex operation that encompasses pre-processing of vectors, partitioning of the database, and encoding of vectors. It is designed to manage and retrieve vector data, support fast and accurate similarity search operations, and enhance the efficiency of storage.<br>\n<br>\nA <b>vector index</b> is a specialized data structure in computer science and information retrieval designed for efficient storage and quick retrieval of high-dimensional vector data. It facilitates rapid similarity searches and nearest neighbor queries, essential for handling complex data sets.<br>\n<br>\nTraditional database queries often search for exact matches by querying scalar indexes for rows or records. However, <b>vector indexes</b> leverage vector embeddings, which contain semantic information, allowing for the search of approximate matches instead. By inputting a vector, the <b>vector index</b> identifies and returns similar vectors, enabling swift searches across extensive vector datasets. This process utilizes a class of algorithms known as Approximate Nearest Neighbor (ANN) search.<br>\n<br>\nTo construct a <b>vector index</b>, a specific similarity metric must be chosen alongside a method for index creation. Commonly used similarity metrics in vector searches are <i>cosine similarity</i>, <i>dot product</i>, and <i>Euclidean distance</i>. <b>Pinecone</b> is employed to facilitate the creation of the index, as demonstrated in the following steps:<br>\nSource: <a href=\"https://www.datastax.com/guides/what-is-a-vector-index\">Vector Indexing Explained: Everything You Need to Know</a><br>\n</span>","metadata":{}},{"cell_type":"code","source":"def create_index(index_name: str, dimension: int, environment: str, metric: str = \"cosine\"):\n    \n    \"\"\"\n    Attempts to create a new Pinecone index with the specified characteristics. If an index\n    with the same name already exists, it does not create a new one and notifies the user.\n    Otherwise, it proceeds to create the index.\n\n    Parameters:\n    - index_name (str): The name of the index to be created. Must be unique within the Pinecone project.\n    - dimension (int): The dimensionality of the vectors that will be stored in the index.\n    - environment (str): The environment setting for the Pinecone index, specifying the hardware\n                         or performance configuration for the index.\n    - metric (str, optional): The distance metric used for vector comparisons within the index.\n                              Defaults to \"cosine\". Other metrics (e.g., \"euclidean\") might be\n                              supported depending on Pinecone's capabilities.\n\n    The function first checks if an index with the specified name already exists by querying the\n    existing indexes. If an index with the given name is found, it prints a message and does not\n    attempt to create a new index. If no such index exists, it uses the provided parameters to\n    create a new index according to the specified dimension, metric, and environment.\n\n    Exceptions during the index creation process are caught and logged, providing feedback about\n    any issues encountered.\n\n    Note:\n    - This function requires a pre-initialized and authenticated Pinecone client (`pc`) to be\n      available in the scope where the function is called.\n    - `PodSpec` is used to specify additional configurations for the Pinecone index, particularly\n      the environment. Make sure the `PodSpec` class is imported and available.\n    \n    Example:\n    >>> create_index(\"my_new_index\", 128, \"gcp-europe-west1\", \"cosine\")\n    \"\"\"\n    \n    try:\n        # Check if the index already exists\n        if index_name in [index[\"name\"] for index in pc.list_indexes()]:\n\n            print(\"An index with the same name already exists!\")\n    \n        else:    \n            # Create the index if it doesn't exist\n            pc.create_index(name=index_name,\n                              dimension=dimension,\n                              metric=metric,\n                              spec=PodSpec(\n                              environment=environment\n                              )\n                            )\n            print(\"Index created successfully\\n\")\n            \n    # Catch any exception and print or log it\n    except Exception as e:\n        print(f\"Index creation failed: {e}\\n\")\n        \n        \n# Create a Pinecone index to upsert the loaded documents\ncreate_index(INDEX_NAME, dim, PINECONE_ENV, metric=\"cosine\")","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:25:20.556979Z","iopub.execute_input":"2024-04-09T05:25:20.557329Z","iopub.status.idle":"2024-04-09T05:25:26.018087Z","shell.execute_reply.started":"2024-04-09T05:25:20.557295Z","shell.execute_reply":"2024-04-09T05:25:26.017044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\n    We've opted for a <i>pod-based index</i>. Given the constraints of the free tier, our options are limited. In setting up a pod-based index, we need to select a cloud environment for hosting. This selection process provides us with a list of available cloud regions and their respective environment parameter values for the <mark>create_index</mark> operation. Under the free Starter plan, we're entitled to one project and one pod-based starter index, equipped with the resources to support up to 100,000 vectors. These starter indexes reside in the <i>gcp-starter</i> environment, specifically in the us-central-1 (Iowa) region on the GCP cloud.<br>\n<br>\nThe effectiveness of certain metrics in terms of recall and precision can vary based on the specific application. We have chosen <i>cosine similarity</i> as our distance metric since it's commonly utilized to ascertain similarities between various documents. A key benefit of using cosine similarity is that it normalizes the scores to a range between -1 and 1, making it a suitable metric for our Generative Question Answering application.<br>\nSource: <a href=\"https://docs.pinecone.io/guides/indexes/understanding-indexes\">Understanding indexes</a><br>\n</span>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\n    <b>Vector stores</b> hold the embedding vectors of ingested document chunks. LlamaIndex offers support for more than 20 different vector store options. We will experiment with <b>Pinecone</b>, previously mentioned as cloud-hosted.\n</span>","metadata":{}},{"cell_type":"code","source":"# Initialize a client instance that targets the index and connect to it\npinecone_index = pc.Index(INDEX_NAME)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:25:26.019464Z","iopub.execute_input":"2024-04-09T05:25:26.019853Z","iopub.status.idle":"2024-04-09T05:25:26.086424Z","shell.execute_reply.started":"2024-04-09T05:25:26.019800Z","shell.execute_reply":"2024-04-09T05:25:26.085521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"text-with-green-strip\">\n<p style=\"font-size: 14px; margin-left: 70px;\">\n    By executing this code, we're essentially setting up a connection to an existing Pinecone index, allowing us to perform various operations such as adding vectors, updating vectors, removing vectors, or performing similarity searches within that index.\n</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\n    Now, it's time to initialize the <b>vector store</b>.\n</span>","metadata":{}},{"cell_type":"code","source":"from llama_index.vector_stores.pinecone import PineconeVectorStore\nfrom llama_index.core import StorageContext\n\n\ndef create_vector_store(vector_index: str) -> object:\n    \n    \"\"\"\n    Initializes and returns a PineconeVectorStore object for interacting with a specific\n    Pinecone index. This function attempts to create a vector store using the provided\n    index name. If successful, it prints a confirmation message. If the operation fails\n    due to an exception, it catches the exception, logs an error message, and still returns\n    the uninitialized or partially initialized vector store object.\n\n    Parameters:\n    - vector_index (str): The name of the Pinecone index to which the vector store will\n      connect. This index should already be created and available in Pinecone.\n\n    Returns:\n    - object: An instance of PineconeVectorStore configured to interact with the specified\n      Pinecone index. The specific type of the returned object is PineconeVectorStore, which\n      allows for operations like inserting, updating, and querying vectors in the index.\n\n    Note:\n    - The function assumes the existence of a class `PineconeVectorStore` which is responsible\n      for managing vector data within a Pinecone index. Make sure this class is imported and\n      available in the environment where the function is called.\n    - The actual connection and interaction with the Pinecone service are dependent on the\n      proper initialization and authentication of the Pinecone client elsewhere in the application.\n\n    Example:\n    >>> vector_store = create_vector_store(\"my_pinecone_index\")\n    >>> # Now, vector_store can be used to perform operations on the \"my_pinecone_index\" index.\n    \"\"\"\n\n    try:\n        vector_store = PineconeVectorStore(pinecone_index=vector_index)\n        \n        print(\"Vector store created successfully\\n\")\n        \n    # Catch any exception and print or log it\n    except Exception as e:\n        print(f\"Vector store creation failed: {e}\\n\")\n        \n    return vector_store\n\n\n# Initialize a Pinecone vector store to store our data\npc_vector_store = create_vector_store(pinecone_index)\npc_storage_context = StorageContext.from_defaults(vector_store=pc_vector_store) ","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:25:26.088242Z","iopub.execute_input":"2024-04-09T05:25:26.088633Z","iopub.status.idle":"2024-04-09T05:25:26.104041Z","shell.execute_reply.started":"2024-04-09T05:25:26.088592Z","shell.execute_reply":"2024-04-09T05:25:26.102890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nIn LlamaIndex, <mark>StorageContext</mark> is a utility container for storing nodes, indices, and vectors.<br>\nSource: <a href=\"https://docs.llamaindex.ai/en/latest/api_reference/storage/storage_context/#llama_index.core.storage.storage_context.StorageContext\">StorageContex</a><br>\n<br>\nBy initializing the index and vector store, we have completed step 8 of the Flowchart.\n</span>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"19\"></a>\n<div class=\"subsection_title\">\n    <a href=\"#table_of_contents\" style=\"color: #46A050;\" >4.9. Inserting Data</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nNow that we have established a connection to the vector database, with both the index and vector store initialized, we are ready to insert our data into the database. The <mark>upsert_data</mark> function is designed to accomplish this task.\n</span>","metadata":{}},{"cell_type":"code","source":"from llama_index.core import VectorStoreIndex\n\n\ndef upsert_data(documents, pc_storage_context, insert_batch_size=1024):\n    \n    \"\"\"\n    Inserts or updates data in a vector store index in batches, creating a new index\n    object based on the provided documents. This operation, known as 'upsert', involves\n    inserting new documents or updating existing ones in the vector store.\n\n    This function leverages a context manager to capture and suppress the output typically\n    generated during the upsert operation, ensuring a cleaner execution output.\n\n    Parameters:\n    - documents: A collection of documents to be upserted into the vector store. The exact\n      format and structure of these documents depend on the requirements of the\n      `VectorStoreIndex.from_documents` method.\n    - pc_storage_context: The storage context associated with the Pinecone vector store\n      where the documents will be upserted. This context should contain necessary\n      configuration and authentication details.\n    - insert_batch_size (int, optional): The size of each batch of documents to be\n      upserted in a single operation. Defaults to 1024, balancing efficiency and\n      resource usage.\n\n    Returns:\n    - A `VectorStoreIndex` object that represents the index containing the upserted\n      documents. This object can be used to interact further with the stored data, such\n      as performing queries or additional updates.\n\n    Note:\n    - The function assumes the existence of a `VectorStoreIndex` class with a method\n      `from_documents` capable of creating or updating an index based on the provided\n      documents and storage context.\n    - The `io.capture_output` context manager is used to suppress output for cleaner\n      execution. Ensure that `io` (likely from the `IPython.utils` module) is correctly\n      imported and available.\n\n    Example:\n    >>> documents = [{...}, {...}]  # Example list of document data\n    >>> pc_storage_context = {...}  # Pinecone storage context configuration\n    >>> index = upsert_data(documents, pc_storage_context)\n    >>> print(index)\n    \"\"\"\n\n    with io.capture_output() as captured:\n        \n        # Upsert the data and create the index object containing the documents\n        index = VectorStoreIndex.from_documents(documents, \n                                                storage_context=pc_storage_context, \n                                                insert_batch_size=insert_batch_size\n                                               )\n\n        return index","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:25:26.105748Z","iopub.execute_input":"2024-04-09T05:25:26.106151Z","iopub.status.idle":"2024-04-09T05:25:26.117351Z","shell.execute_reply.started":"2024-04-09T05:25:26.106114Z","shell.execute_reply":"2024-04-09T05:25:26.116314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\n<mark>VectorStoreIndex</mark> saves nodes in the form of vector embeddings. LlamaIndex provides the functionality to keep these vector embeddings either locally or in a specialized vector database such as Pinecone. Upon receiving a query, LlamaIndex identifies and returns the <mark>top_k</mark> most similar nodes to the response synthesizer.<br>\n<br>\nImplementing a vector store index allows for the incorporation of similarity searches into our LLM application, making it the ideal choice for workflows that involve comparing textual content for semantic similarity through vector searches. For instance, to pose questions regarding a particular kind of open-source software, employing a vector store index would be the most effective strategy.<br>\nSource: <a href=\"https://zilliz.com/blog/getting-started-with-llamaindex#Vector-Store-Index\">Vector Store Index</a><br>\n<br>\n    When we utilize <mark>from_documents</mark>, our Documents are divided into chunks and converted into Node objects. These Nodes are lightweight abstractions of text strings that also maintain metadata and relationships. By default, <mark>VectorStoreIndex</mark> processes and inserts vectors in batches of 2048 nodes. If memory limitations are a concern, we can adjust this default by specifying a different <mark>insert_batch_size=2048</mark> value according to our needs. Due to our memory constraints, we will adjust this parameter to <i>1024</i>.<br>\nSource: <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/#using-vectorstoreindex\">Using VectorStoreIndex</a><br>\n<br>\nIn this way, step 9 of the Flowchart is completed.\n</span>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"20\"></a>\n<div class=\"subsection_title\">\n    <a href=\"#table_of_contents\" style=\"color: #46A050;\" >4.10. Creating the Query Engine</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nIn the ingestion pipeline, after loading the data, the next steps were transforming, indexing, and then storing the data, as previously mentioned. We needed to process and transform our data before storing it. These transformations include chunking the data, extracting metadata, and embedding each chunk. This ensures that the data can be efficiently retrieved and effectively utilized by the LLM. By now, we have completed these final stages.<br>    \n<br>\nNext, we will set up prompts. Prompts are essential inputs that empower LLMs with their versatility. LlamaIndex leverages prompts for building indexes, inserting data, navigating during queries, and synthesizing final answers.<br>\nSource: <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/models/prompts/#prompts\">Prompts</a><br>\n<br>\nA system prompt serves as a means to offer context, instructions, and guidelines to a data framework prior to posing a question or task. This approach effectively prepares the ground for the interaction, defining the data framework's role, personality, tone, and any additional specifics that aid in its comprehension and reaction to user prompts.<br>\n<br>\nLlamaIndex comes equipped with a selection of default prompt templates that are effective right away. We will choose one of these default templates. The <mark>text_qa_template</mark> and <mark>refine_template</mark> are among the most frequently used prompts.<br>\nSource: <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/models/prompts/usage_pattern/#commonly-used-prompts\">Commonly Used Prompts</a><br>\n<br>\nFor our system prompts, we will be using the following templates.:<br>\nSource: <a href=\"https://docs.llamaindex.ai/en/stable/examples/customization/prompts/completion_prompts.html\">Completion Prompts Customization</a><br>\n</span>","metadata":{}},{"cell_type":"code","source":"from llama_index.core import PromptTemplate\n\n\ntext_qa_template_str = (\n    \"Context information is\"\n    \" below.\\n---------------------\\n{context_str}\\n---------------------\\nUsing\"\n    \" both the context information and also using your own knowledge, answer\"\n    \" the question: {query_str}\\nIf the context isn't helpful, you can also\"\n    \" answer the question on your own.\\n\"\n)\ntext_qa_template = PromptTemplate(text_qa_template_str)\n\nrefine_template_str = (\n    \"The original question is as follows: {query_str}\\nWe have provided an\"\n    \" existing answer: {existing_answer}\\nWe have the opportunity to refine\"\n    \" the existing answer (only if needed) with some more context\"\n    \" below.\\n------------\\n{context_msg}\\n------------\\nUsing both the new\"\n    \" context and your own knowledge, update or repeat the existing answer.\\n\"\n)\nrefine_template = PromptTemplate(refine_template_str)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:25:26.118637Z","iopub.execute_input":"2024-04-09T05:25:26.119000Z","iopub.status.idle":"2024-04-09T05:25:26.131483Z","shell.execute_reply.started":"2024-04-09T05:25:26.118972Z","shell.execute_reply":"2024-04-09T05:25:26.130393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nAs said, the most commonly used prompts are <mark>text_qa_template</mark> and the <mark>refine_template</mark>.\n</span>\n<ul>\n    <li><mark>text_qa_template</mark>: used to get an initial answer to a query using retrieved nodes</li>\n    <li><mark>refine_template</mark>: used when the retrieved text does not fit into a single LLM call with <mark>response_mode=\"compact\"</mark> (the default), or when more than one node is retrieved using <mark>response_mode=\"refine\"</mark>. The answer from the first query is inserted as an <mark>existing_answer</mark>, and the LLM must update or repeat the existing answer based on the new context.</li>\n</ul>\n<span style=\"font-size:medium;\">\nSource: <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/models/prompts/usage_pattern/#commonly-used-prompts\">Commonly Used Prompts</a><br>\n</span>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nNext, we need to design the query engines. A <b>query engine</b> is a universal interface designed for posing questions about our data. It processes queries in natural language and delivers detailed responses. Typically, though not exclusively, it operates using one or more indexes through retrievers. Our query engine will use our fine-tuned <b>Gemma</b> as the LLM.<br>\nSource: <a href=\"https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/root.html\">Query Engine</a><br>\n<br>\n</span>","metadata":{}},{"cell_type":"code","source":"def create_query_engine(index, llm, text_qa_template=None, refine_template=None, similarity_top_k=None):\n    \n    \"\"\"\n    Creates a query engine with or without question-answering and refinement templates.\n    The engine is configured to use a specified language model (llm) and can optionally\n    consider a specified number of top similar results during query processing.\n\n    This function supports creating two types of query engines based on the presence\n    of text question-answering and refinement templates:\n    1. Without templates: If both templates are None, it creates a basic query engine.\n    2. With templates: If both templates are provided, it creates an advanced query engine\n       that utilizes these templates for processing.\n\n    Parameters:\n    - index: The index object to which the query engine will be attached. This object\n      should support the `as_query_engine` method.\n    - llm: The language model to be used by the query engine for generating responses.\n    - text_qa_template (optional): A template for structuring questions to the language\n      model, enhancing question-answering capabilities.\n    - refine_template (optional): A template for refining responses from the language model,\n      improving the relevance and accuracy of answers.\n    - similarity_top_k (optional): Specifies the number of top similar results to consider\n      when processing queries. Useful for narrowing down responses to the most relevant ones.\n\n    Returns:\n    - The created query engine object, ready for executing queries against the specified index\n      using the configured language model and optional templates.\n\n    Note:\n    - The function attempts to create the query engine and reports success or failure. Any\n      exceptions encountered during creation are caught and logged.\n    - The actual capabilities and options of the query engine depend on the implementation\n      of the `as_query_engine` method in the `index` object.\n\n    Example:\n    >>> index = ...  # An example index object\n    >>> llm = ...    # An example language model\n    >>> query_engine = create_query_engine(index, llm, text_qa_template=\"How do I {}?\",\n    ...                                    refine_template=\"The answer to your question is {}\",\n    ...                                    similarity_top_k=5)\n    >>> print(query_engine)\n    \"\"\"\n    \n    try:\n        # Create an engine without templates\n        if text_qa_template is None or refine_template is None:\n            query_engine = index.as_query_engine(llm=llm, similarity_top_k=similarity_top_k)\n\n        # Create an engine with templates\n        elif text_qa_template is not None and refine_template is not None:\n            query_engine = index.as_query_engine(text_qa_template=text_qa_template,\n                                                    refine_template=refine_template,\n                                                    llm=llm,\n                                                )\n            \n        print(\"Query engine created successfully\\n\")\n        \n        # Catch any exception and print or log it\n    except Exception as e:\n        print(f\"Query engine creation failed: {e}\\n\")\n    \n    return query_engine","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:25:26.139068Z","iopub.execute_input":"2024-04-09T05:25:26.139478Z","iopub.status.idle":"2024-04-09T05:25:26.148689Z","shell.execute_reply.started":"2024-04-09T05:25:26.139446Z","shell.execute_reply":"2024-04-09T05:25:26.147726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\n    Successful execution of the <mark>create_query_engine</mark> function completes step 10 and subsequently carries out the tasks outlined in steps 11, 12a, and 12b.\n</span>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"21\"></a>\n<div class=\"subsection_title\">\n    <a href=\"#table_of_contents\" style=\"color: #46A050;\" >4.11. Displaying Responses and Relevancy Evaluation</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nNow that we have made a RAG app and queried our LLM, we need to evaluate its response.<br>\n<br>\nWith LlamaIndex, there are many ways to evaluate the results our RAG app generates. A great way to get started with evaluation is to confirm (or deny) that our LLM’s responses are relevant, given the context retrieved from our vector database. To do this, we can use LlamaIndex’s <mark>RelevancyEvaluator</mark> class.<br>\n<br>\nThe great thing about this type of evaluation is that there is no need for ground truth data (i.e., labeled datasets to compare answers with).<br>\nSource: <a href=\"https://docs.pinecone.io/integrations/llamaindex#evaluate-the-data\">Evaluate the data</a><br>\n</span>","metadata":{}},{"cell_type":"code","source":"from llama_index.core.evaluation import RelevancyEvaluator\n\n# (Need to avoid peripheral asyncio issues)\nimport nest_asyncio\nnest_asyncio.apply()\n\n\n# Define evaluator\nevaluator = RelevancyEvaluator(llm=gemma_llama)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:25:26.149879Z","iopub.execute_input":"2024-04-09T05:25:26.150198Z","iopub.status.idle":"2024-04-09T05:25:26.205155Z","shell.execute_reply.started":"2024-04-09T05:25:26.150169Z","shell.execute_reply":"2024-04-09T05:25:26.204120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"text-with-green-strip\">\n<p style=\"font-size: 14px; margin-left: 70px;\">\n    This code snippet sets up an environment that allows asynchronous operations to run smoothly in a Jupyter notebook (or similar environment) and initializes an evaluator for assessing the relevancy of outputs based on the <span class=\"marks\">gemma_llama</span> language model.\n</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\n    The following <mark>display_eval_df</mark> function prints the evaluation results to the screen.\n</span>","metadata":{}},{"cell_type":"code","source":"from llama_index.core.evaluation import EvaluationResult\n\n\ndef display_eval_df(query: str, response: str, eval_result: EvaluationResult) -> None:\n    \n    \"\"\"\n    Displays an evaluation DataFrame in a Jupyter Notebook or similar environment, \n    summarizing the results of evaluating a given response against a query. This \n    DataFrame includes the query, the response, a snippet of the source text used \n    to generate the response, the overall evaluation result (Pass or Fail), and \n    any reasoning provided for the evaluation.\n\n    Parameters:\n    - query (str): The original query text for which the response was generated.\n    - response (str): The response text generated for the query. It is converted to \n      string within the function, so it can also handle response objects if necessary.\n    - eval_result (EvaluationResult): An object containing the results of the \n      evaluation, including whether the response passed or failed and any feedback \n      or reasoning associated with the evaluation.\n\n    Note:\n    - The `response` parameter is expected to have a `source_nodes` attribute, which \n      is a list. The function uses the first item in this list to extract and display \n      a snippet of the source text. Ensure the response object structure is consistent \n      with this expectation.\n    - `EvaluationResult` is a custom object type that should have `passing` and \n      `feedback` attributes. `passing` is a boolean indicating the success of the \n      evaluation, and `feedback` is a string providing details or reasons for the \n      evaluation outcome.\n    - This function is designed to be used in Jupyter Notebooks or other IPython \n      environments that support the `display` function for rendering DataFrames.\n\n    The function first creates a pandas DataFrame with a single row summarizing the \n    evaluation details. It then applies styling to ensure the response and source \n    text are appropriately wrapped for readability. Finally, it uses IPython's \n    `display` function to render the styled DataFrame in the notebook.\n\n    Example:\n    >>> eval_result = EvaluationResult(passing=True, feedback=\"The response is accurate.\")\n    >>> display_eval_df(\"What is KeyBERT?\", \"KeyBERT is a keyword extraction tool.\", eval_result)\n    \"\"\"\n    \n    eval_df = pd.DataFrame(\n        {\n            \"Query\": query,\n            \"Response\": str(response),\n            \"Source\": response.source_nodes[0].node.text[:1000] + \"...\",\n            \"Evaluation Result\": \"Pass\" if eval_result.passing else \"Fail\",\n            \"Reasoning\": eval_result.feedback,\n        },\n        index=[0],\n    )\n    eval_df = eval_df.style.set_properties(\n        **{\n            \"inline-size\": \"600px\",\n            \"overflow-wrap\": \"break-word\",\n        },\n        subset=[\"Response\", \"Source\"]\n    )\n    display(eval_df)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:25:26.206526Z","iopub.execute_input":"2024-04-09T05:25:26.206878Z","iopub.status.idle":"2024-04-09T05:25:26.216387Z","shell.execute_reply.started":"2024-04-09T05:25:26.206822Z","shell.execute_reply":"2024-04-09T05:25:26.215304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\n    The relevancy evaluation results printed to the screen are generated by the following <mark>evaluate_response</mark> function.\n</span>","metadata":{}},{"cell_type":"code","source":"def evaluate_response(query: str, query_engine) -> None:\n\n    \"\"\"\n    Evaluates a response to a given query using a specified query engine, then displays\n    the evaluation results. This function sends a query to the query engine, receives the\n    response, and evaluates the response using a predefined evaluator. The evaluation\n    results, along with the query and response, are then displayed in a formatted table.\n\n    Parameters:\n    - query (str): The query text to be evaluated.\n    - query_engine: An object capable of executing queries and returning responses. It is\n      expected to have a `query` method that accepts a query string and returns a response\n      object.\n\n    The function operates within a context manager to suppress output generated during the\n    query and evaluation process, focusing on displaying only the formatted evaluation results\n    at the end. The response's source nodes are processed to extract content, which, along with\n    the query and the evaluation result, is displayed using the `display_eval_df` function.\n\n    Note:\n    - The response object returned by `query_engine.query` is expected to have a `source_nodes`\n      attribute, containing nodes that the response is based on. Each node should have a\n      `get_content` method to extract textual content.\n    - `evaluator.evaluate_response` is a function (assumed to be available in the scope) that\n      takes a query and a response object as input and returns an evaluation result. The\n      evaluation result should include whether the response passes certain criteria and any\n      feedback or reasoning.\n    - This function assumes the presence of `io.capture_output` for output suppression and\n      `display_eval_df` for displaying evaluation results. Ensure these are imported and\n      available in your environment.\n\n    Example:\n    >>> query = \"What is the capital of France?\"\n    >>> query_engine = MyQueryEngine()\n    >>> evaluate_response(query, query_engine)\n    \n    This example sends a query to `MyQueryEngine` (a hypothetical query engine) and displays\n    the evaluation results of the response.\n    \"\"\"\n    \n    with io.capture_output() as captured:\n        \n        # Issue query\n        llm_response = query_engine.query(query)\n\n        # Grab context used in answer query & make it pretty\n        llm_response_source_nodes = [i.get_content() for i in llm_response.source_nodes]\n\n        # Take your previous question and pass in the response you have got above\n        eval_result = evaluator.evaluate_response(query=query, response=llm_response)\n\n    # Display response\n    display_eval_df(query, llm_response, eval_result)\n\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:25:26.217939Z","iopub.execute_input":"2024-04-09T05:25:26.218330Z","iopub.status.idle":"2024-04-09T05:25:26.231612Z","shell.execute_reply.started":"2024-04-09T05:25:26.218294Z","shell.execute_reply":"2024-04-09T05:25:26.230690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nThis section finishes the tasks outlined in steps 13 and 14 of the Flowchart.\n</span>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"22\"></a>\n<div class=\"subsection_title\">\n    <a href=\"#table_of_contents\" style=\"color: #46A050;\" >4.12. Handling Queries and Generating Responses</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nNow, we are prepared to generate responses with the query engine. Upon receiving a query, a response will be generated and subsequently refined for proper display. The <mark>handle_query</mark> function will manage query handling and response generation, while the <mark>clean_response</mark> function will take care of cleaning and refining the response text. Finally, the <mark>output_response</mark> function will display the results in a readable format on the screen. In generating responses, our fine-tuned <b>Gemma</b> will take the lead as the Large Language Model (LLM). These three functions are presented below in the order mentioned.<br>\n<br>\nBy deleting the index and query engine in the final execution lines of the last two functions, we will complete step 15 of the Flowchart, thereby concluding the RAG cycle.\n</span>","metadata":{}},{"cell_type":"code","source":"def clean_response(text: str) -> str:\n\n    \"\"\"\n    Cleans up a given text string by performing a series of transformations aimed at\n    improving readability and removing unnecessary elements.\n\n    The transformations applied are as follows:\n    1. Replace newline characters ('\\n') with a space to maintain text continuity.\n    2. Remove long sequences of dashes (\"---------------------\") often used as separators.\n    3. Remove the specific phrase 'Context information is below.' which may be redundant.\n    4. Trim leading and trailing whitespace to clean up the text.\n    5. Extract and keep only complete sentences, removing any incomplete sentence at the end\n       of the text. A complete sentence is defined as any sequence of characters ending with\n       a period ('.') followed by a whitespace or the end of the string.\n    6. Rejoin the extracted complete sentences into a single string for the final cleaned text.\n\n    Parameters:\n    - text (str): The original text string to be cleaned.\n\n    Returns:\n    - str: The cleaned text after applying all the specified transformations.\n\n    Note:\n    - This function uses regular expressions (re.findall) to identify and extract complete\n      sentences. It assumes that periods ('.') are reliable sentence terminators and may not\n      account for all nuances of natural language punctuation and usage.\n\n    Example:\n    >>> original_text = \"Here is a sentence. And here is an incomplete sentence---\\\\nContext information is below.\"\n    >>> clean_response(original_text)\n    'Here is a sentence. And here is an incomplete sentence'\n    \"\"\"\n\n    # Step 1: Replace \\n characters with a space\n    text = text.replace(\"\\n\", \" \")\n\n    # Step 2: Remove \"---\"\n    text = text.replace(\"---------------------\", \"\")\n    \n    # Step 3: Remove the phrase 'Context information is below.'\n    text = text.replace(\"Context information is below.\", \"\")\n    \n    # Step 4: Remove empty spaces\n    text = text.strip()\n        \n    # Step 5: Remove uncomplete sentences in the end\n    complete_sentences = re.findall(r'.*?\\.\\s?', text)\n    \n    # Step 6: Join the matched complete sentences back into a single string\n    cleaned_text = ''.join(complete_sentences)\n\n    return cleaned_text","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:25:26.232973Z","iopub.execute_input":"2024-04-09T05:25:26.233313Z","iopub.status.idle":"2024-04-09T05:25:26.246752Z","shell.execute_reply.started":"2024-04-09T05:25:26.233276Z","shell.execute_reply":"2024-04-09T05:25:26.245898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def handle_query(query: str, gemma_llama, gemma_llama_pc_dict: dict) -> tuple:\n    \n    \"\"\"\n    Processes a given query by extracting relevant keywords, loading documents for those\n    keywords, upserting document data into a Pinecone index, and retrieving a response\n    based on the latest data. The function also performs cleanup after data processing.\n\n    Steps involved in processing the query:\n    1. Extract keywords from the query based on a specified similarity threshold.\n    2. Filter keywords to include only those that can be loaded without errors (e.g., from Wikipedia).\n    3. Create and load documents for every successfully loaded keyword.\n    4. Upsert the document data into a Pinecone index.\n    5. Create a new query engine configured with the updated index.\n    6. Retrieve and format the response to the query.\n    7. Record the formatted response in a provided dictionary.\n    8. Perform cleanup by deleting temporary objects and collecting garbage.\n\n    Parameters:\n    - query (str): The query string to be processed.\n    - gemma_llama: An object or reference required for creating the query engine. The specific\n      type and purpose should align with the creation of a query engine in the function implementation.\n    - gemma_llama_pc_dict (dict): A dictionary where query responses are recorded. Keys are queries,\n      and values are the corresponding responses.\n\n    Returns:\n    - tuple: A tuple containing the updated dictionary with responses, the cleaned response to\n      the current query, and the query engine used for retrieving the response.\n\n    Note:\n    - This function assumes the presence and correct initialization of several functions and objects\n      for processing steps, including `process_keywords`, `try_loading_keyword`, `create_doc`,\n      `upsert_data`, `create_query_engine`, and `clean_response`. Each of these should be defined\n      and available in the scope.\n    - Proper error handling, context management, and cleanup are performed to ensure efficient\n      and error-free processing.\n\n    Example:\n    >>> query = \"What is deep learning?\"\n    >>> gemma_llama = SomeModelOrObjectNeededForQueryEngine()\n    >>> gemma_llama_pc_dict = {}\n    >>> updated_dict, cleaned_response, query_engine = handle_query(query, gemma_llama, gemma_llama_pc_dict)\n    >>> print(cleaned_response)\n    \"\"\"\n    \n    # Initialize an empty list for keywords that can be loaded without errors\n    successful_keywords = []  \n    \n    # Extract keywords from the query\n    keywords = process_keywords(query, threshold=0.5)\n\n    # Include only keywords that are found in Wikipedia\n    # Iterate through each keyword and add it to the successful list if it doesn't raise an error\n    for keyword in keywords:\n        if try_loading_keyword(keyword):\n            successful_keywords.append(keyword)\n\n    # Create and load Wikipedia documents for every succesful keyword\n    documents = create_doc(successful_keywords) \n\n    # Upsert the data to the index\n    index = upsert_data(documents, pc_storage_context, insert_batch_size=1024)\n\n    # Create a new query engine to search the response in the latest data upserted\n    query_engine = create_query_engine(index, gemma_llama, similarity_top_k=1)\n#     query_engine = create_query_engine(index, gemma_llama, text_qa_template=text_qa_template, refine_template=refine_template)\n\n    # Retrieve the response to the query\n    # https://docs.llamaindex.ai/en/stable/module_guides/indexing/index_guide/\n    with io.capture_output() as captured:\n        response_body = query_engine.query(query)\n    response = response_body.response\n\n    # Format the response\n    cleaned_response = clean_response(response)\n\n    # Record responses into the dictionary\n    gemma_llama_pc_dict[query] = cleaned_response\n\n    # Delete stuff to prepare for future queries\n    del index, response, successful_keywords, documents\n    gc.collect()\n\n    return gemma_llama_pc_dict, cleaned_response, query_engine","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:25:26.248003Z","iopub.execute_input":"2024-04-09T05:25:26.248289Z","iopub.status.idle":"2024-04-09T05:25:26.262872Z","shell.execute_reply.started":"2024-04-09T05:25:26.248265Z","shell.execute_reply":"2024-04-09T05:25:26.262109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def output_response(query: str, query_engine, cleaned_response: str) -> None:\n    \n    \"\"\"\n    Outputs the query and its cleaned response in a formatted manner, evaluates the\n    response quality using a predefined evaluation function, and displays a visual\n    separator. This function is designed to enhance readability and structure in the\n    presentation of query processing results.\n\n    Parameters:\n    - query (str): The original query text.\n    - query_engine: The query engine used to obtain the response. This engine is also\n      passed to the evaluation function to assess response quality.\n    - cleaned_response (str): The cleaned and formatted response text to be displayed.\n\n    The function uses ANSI escape codes to colorize the output for \"Query\" and \"Response\"\n    labels, as well as \"Evaluation\" when printing to the console. It concludes by displaying\n    a green horizontal strip as a visual separator in the output, aiming to distinguish\n    between different sections or responses visually when displayed in a Jupyter Notebook\n    or other IPython environments.\n\n    Note:\n    - This function assumes the presence of an `evaluate_response` function for assessing\n      response quality and that `display` and `HTML` are available for rendering HTML content.\n      Ensure all dependencies are properly imported.\n    - The function deletes the `query_engine` and `cleaned_response` variables at the end,\n      which is intended to free up resources. Be mindful of this if you plan to use these\n      variables after calling this function.\n\n    Example:\n    >>> query = \"Explain deep learning\"\n    >>> query_engine = YourQueryEngine()  # Example placeholder\n    >>> cleaned_response = \"Deep learning is a subset of machine learning...\"\n    >>> output_response(query, query_engine, cleaned_response)\n    \n    This example will print the query and response, call the evaluation function, and\n    display the green strip, assuming all components are correctly set up.\n    \"\"\"\n    \n    # ANSI escape codes for colors\n    MAGENTA = '\\033[95m' # Color code for magenta\n    ENDC = '\\033[0m'  # Resets color to default\n\n    # Print the query and response\n    print(\"\\n\", MAGENTA + \"Query:\" + ENDC, \"\\n\", query, \"\\n\\n\", MAGENTA + \"Response:\" + ENDC, \"\\n\", cleaned_response, \"\\n\\n\")\n\n    # Evaluate the response quality\n    print(\"\\n\", MAGENTA + \"Evaluation:\" + ENDC, \"\\n\")\n    evaluate_response(query, query_engine)\n    \n    # Define the HTML content for the green horizontal strip\n    green_strip_html = \"\"\"\n    <div style=\"background-color: #A0FFA1; width: 100%; height: 20px; margin: 20px 0;\"></div>\n    \"\"\"\n    \n    # Display the green strip in the output\n    display(HTML(green_strip_html))\n    \n    del query_engine, cleaned_response","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:25:26.263949Z","iopub.execute_input":"2024-04-09T05:25:26.264215Z","iopub.status.idle":"2024-04-09T05:25:26.277881Z","shell.execute_reply.started":"2024-04-09T05:25:26.264192Z","shell.execute_reply":"2024-04-09T05:25:26.277100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nNow, we can run the entire code in this section of the notebook.\n</span>","metadata":{}},{"cell_type":"code","source":"# Create an empty dictionary to store queries and responses\ngemma_llama_pc_dict = {}\n\n# Loop over queries received\nfor query in queries:\n\n    # Initialize the attempt counter\n    n = 0\n\n    # More accurate results\n    for i in range(5):\n        # Query again if no answer is received\n        while n < 5:\n            with io.capture_output() as captured:\n                try:\n                    gemma_llama_pc_dict, cleaned_response, query_engine = handle_query(query, gemma_llama, gemma_llama_pc_dict)\n                    break  # Exit the loop after a successful query\n                except Exception as e:\n                    print(f\"Attempt {n + 1} failed with error: {e}\")\n                    n += 1  # Increment the counter only if an exception occurs\n\n    # Output response           \n    output_response(query, query_engine, cleaned_response)\n                \n# Add the responses to the general store\nMODELS_DICT[\"Gemma_with_LLamaIndex_and_Pinecone\"] = gemma_llama_pc_dict","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:25:26.278874Z","iopub.execute_input":"2024-04-09T05:25:26.279146Z","iopub.status.idle":"2024-04-09T05:29:06.919240Z","shell.execute_reply.started":"2024-04-09T05:25:26.279116Z","shell.execute_reply":"2024-04-09T05:29:06.918329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"text-with-green-strip\">\n<p style=\"font-size: 14px; margin-left: 70px;\">\n    The above code snippet outlines a process for handling a list of queries using a specific query handling mechanism and recording the responses. Here's a step-by-step breakdown of what it does:<br>\n    1. <b>Initialize a Dictionary</b>: An empty dictionary <span class=\"marks\">gemma_llama_pc_dict</span> is created to store the queries and their corresponding responses.<br>\n    2. <b>Loop Over Queries</b>: The script loops over each query in a list `queries`.<br>\n    3. <b>Attempt Counter Initialization</b>: For each query, it initializes an attempt counter <span class=\"marks\">n</span> to 0, which is used to track the number of attempts made to handle the query successfully.<br>\n    4. <b>Query Handling Attempts</b>:<br>\n   - For each query, it attempts to handle the query up to 5 times (controlled by <span class=\"marks\">for i in range(5):</span>). This is intended to ensure more accurate results by potentially repeating the query handling process.<br>\n   - Within each of these attempts, it tries up to 5 times (controlled by <span class=\"marks\">while n < 5: </span>) to get a successful response. This inner loop accounts for possible exceptions during query handling, possibly due to communication errors, timeouts, or other issues.<br>\n   - The <span class=\"marks\">handle_query</span> function is called with the current query, <span class=\"marks\">gemma_llama</span>, and the <span class=\"marks\">gemma_llama_pc_dict</span>. This function is expected to handle the query, by fetching data, evaluating the query, or generating a response, and then updating <span class=\"marks\">gemma_llama_pc_dict</span> with the query-response pair.<br>\n   - If <span class=\"marks\">handle_query</span> executes successfully, it breaks out of the inner loop. If an exception occurs, it prints the error and retries until the maximum number of attempts (5) is reached.<br>\n    5. <b>Output Response</b>: After handling a query, whether successfully or after exhausting retries, it calls <span class=\"marks\">output_response</span> with the current query, the <span class=\"marks\">query_engine</span>, and the <span class=\"marks\">cleaned_response</span> to output the query and its response in a formatted manner, evaluate the response quality, and display a visual separator.<br>\n    6. <b>Update General Store</b>: After processing all queries, it updates a dictionary <span class=\"marks\">MODELS_DICT</span> with the key <span class=\"marks\">\"Gemma_with_LLamaIndex_and_Pinecone\"</span> and assigns <span class=\"marks\">gemma_llama_pc_dict</span> as its value. This seems to be a way to store the responses from this specific query handling session in a larger collection of models or response sources.<br>\n</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nFinally, we incorporate the responses into the general collection for display and comparison purposes.<br>\n</span>","metadata":{}},{"cell_type":"code","source":"# Display the cumulative list of models and their outputs\ncreate_df_from_dicts(\n                    MODELS_DICT, \n                    max_width_px=350, \n                    min_height_px=25, \n                    even_row_color=\"#BEFFBF\", \n                    odd_row_color=\"#82E183\"\n                )","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:30:30.083052Z","iopub.execute_input":"2024-04-09T05:30:30.083779Z","iopub.status.idle":"2024-04-09T05:30:30.111090Z","shell.execute_reply.started":"2024-04-09T05:30:30.083748Z","shell.execute_reply":"2024-04-09T05:30:30.109880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nAs evident from the responses, the model utilizes RAG to fetch answers from web pages.<br>\n<br>\nWhen enhanced with RAG, the model is able to retrieve answers to questions, provided those answers exist within the source documents. In cases where the answers are not in the source documents, the model might produce responses on its own.<br>\n<br>\nThe fine-tuned model produces highly accurate and concise results. When augmented by RAG, it retrieves responses from <b>Wikipedia</b>, which it utilizes as its knowledge base.<br>\n<br>\n    If we prefer, we can delete the index in <b>Pinecone</b>:\n</span>","metadata":{}},{"cell_type":"code","source":"# Delete the index\npc.delete_index(INDEX_NAME)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T05:30:41.226983Z","iopub.execute_input":"2024-04-09T05:30:41.227746Z","iopub.status.idle":"2024-04-09T05:30:41.254278Z","shell.execute_reply.started":"2024-04-09T05:30:41.227716Z","shell.execute_reply":"2024-04-09T05:30:41.252967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"23\"></a>\n<div class=\"section_title\">\n    <a href=\"#table_of_contents\" style=\"color: white;\" >5. Conclusion</a>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"font-size:medium;\">\nInitially, we utilized the pretrained version of Gemma to answer our questions. Subsequently, we fine-tuned the model using a custom dataset. Finally, we integrated RAG with LlamaIndex to broaden the response base for the queries. Each of these steps has yielded results with varying degrees of accuracy.<br>\n<br>\nIn the future, it's possible to use and fine-tune the 7B model, which may require a more extensive dataset. Any RAG framework could be more effectively integrated with the LLM model to yield more relevant and precise results. With LlamaIndex, there's potential for further expansion, leveraging new features and incorporating a wider variety of documents in different formats.\n</span>","metadata":{}}]}